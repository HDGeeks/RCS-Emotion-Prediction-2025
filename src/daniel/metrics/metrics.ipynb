{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac65636b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ cwd          : /Users/hd/Desktop/RCS-Emotion-Prediction-2025/src/daniel/metrics\n",
      "ðŸ“‚ Project root : /Users/hd/Desktop/RCS-Emotion-Prediction-2025\n",
      "ðŸ“‚ Source root  : /Users/hd/Desktop/RCS-Emotion-Prediction-2025/src/daniel/gemini\n",
      "ðŸ“‚ Data root    : /Users/hd/Desktop/RCS-Emotion-Prediction-2025/data/MAMS-ACSA/raw/data_jsonl/annotated\n",
      "ðŸ“‚ Prompts root : /Users/hd/Desktop/RCS-Emotion-Prediction-2025/prompts/daniel/llama\n",
      "ðŸ“‚ Utils root   : /Users/hd/Desktop/RCS-Emotion-Prediction-2025/utils\n",
      "ðŸ“‚ Results root : /Users/hd/Desktop/RCS-Emotion-Prediction-2025/results/daniel\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "PROJECT_MARKERS = (\"src\", \"data\", \"prompts\", \"results\")\n",
    "\n",
    "def find_project_root(start_path):\n",
    "    current = os.path.abspath(start_path)\n",
    "\n",
    "    while True:\n",
    "        if all(os.path.isdir(os.path.join(current, m)) for m in PROJECT_MARKERS):\n",
    "            return current\n",
    "\n",
    "        parent = os.path.dirname(current)\n",
    "        if parent == current:\n",
    "            raise RuntimeError(\"Project root not found\")\n",
    "\n",
    "        current = parent\n",
    "\n",
    "\n",
    "# ---- execution directory (cwd) ----\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# ---- safe starting point ----\n",
    "try:\n",
    "    start_path = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    start_path = cwd\n",
    "\n",
    "\n",
    "# ---- resolve canonical paths ----\n",
    "project_root = find_project_root(start_path)\n",
    "\n",
    "# âœ… THIS IS THE IMPORTANT PART\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "src_root     = os.path.join(project_root, \"src\", \"daniel\", \"gemini\")\n",
    "data_root    = os.path.join(project_root, \"data\", \"MAMS-ACSA\", \"raw\", \"data_jsonl\", \"annotated\")\n",
    "schemas_root = os.path.join(project_root, \"data\", \"MAMS-ACSA\", \"raw\", \"data_jsonl\", \"schema\")\n",
    "prompts_root = os.path.join(project_root, \"prompts\", \"daniel\", \"llama\")\n",
    "utils_root   = os.path.join(project_root, \"utils\")\n",
    "results_root = os.path.join(project_root, \"results\", \"daniel\")\n",
    "\n",
    "print(\n",
    "    f\"ðŸ“‚ cwd          : {cwd}\\n\"\n",
    "    f\"ðŸ“‚ Project root : {project_root}\\n\"\n",
    "    f\"ðŸ“‚ Source root  : {src_root}\\n\"\n",
    "    f\"ðŸ“‚ Data root    : {data_root}\\n\"\n",
    "    f\"ðŸ“‚ Prompts root : {prompts_root}\\n\"\n",
    "    f\"ðŸ“‚ Utils root   : {utils_root}\\n\"\n",
    "    f\"ðŸ“‚ Results root : {results_root}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8741d5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded gold rows: 299\n",
      "Loaded pred rows: 299\n",
      "\n",
      "âœ… No triple-count mismatches found\n",
      "Used 583 aligned emotion labels for evaluation\n",
      "\n",
      "==============================\n",
      "EMOTION-ONLY EVALUATION REPORT\n",
      "==============================\n",
      "\n",
      "Emotion Macro-F1      : 0.4141\n",
      "Emotion Micro-F1      : 0.4957\n",
      "Exact Emotion Accuracy: 0.4957\n",
      "\n",
      "==============================\n",
      "DETAILED EMOTION REPORT\n",
      "\n",
      "==============================\n",
      "emotion           precision    recall  f1-score   support\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "    admiration       0.48      0.59      0.53        74\n",
      "     annoyance       0.50      0.74      0.60       119\n",
      "disappointment       0.44      0.33      0.38       104\n",
      "       disgust       0.50      0.29      0.36         7\n",
      "     gratitude       0.43      0.33      0.38         9\n",
      "mentioned_only       0.61      0.59      0.60        90\n",
      "mixed_emotions       0.20      0.08      0.11        13\n",
      "    no_emotion       0.32      0.21      0.25        67\n",
      "  satisfaction       0.54      0.50      0.52       100\n",
      "\n",
      "      accuracy                           0.50       583\n",
      "     macro avg       0.45      0.41      0.41       583\n",
      "  weighted avg       0.48      0.50      0.48       583\n",
      "\n",
      "\n",
      "Saved â†’ f1_metrics_gpt_output.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# EMOTION-ONLY EVALUATION WITH MISMATCH LOGGING\n",
    "# ============================================================\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# -----------------------------\n",
    "# Paths\n",
    "# -----------------------------\n",
    "\n",
    "GOLD_PATH = os.path.join(data_root, \"02_iteration_cleaned_300.jsonl\")\n",
    "#PRED_PATH = os.path.join(results_root, \"llama\", \"llama_output.jsonl\")\n",
    "#PRED_PATH = os.path.join(results_root, \"claude\", \"claude_output.jsonl\")\n",
    "PRED_PATH = os.path.join(results_root, \"gpt\", \"gpt_output.jsonl\")\n",
    "\n",
    "# ---- derive input identifier ----\n",
    "input_name = os.path.splitext(os.path.basename(PRED_PATH))[0] # prints model\n",
    "\n",
    "# -----------------------------\n",
    "# Load JSONL\n",
    "# -----------------------------\n",
    "def load_jsonl(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "gold = load_jsonl(GOLD_PATH)\n",
    "pred = load_jsonl(PRED_PATH)\n",
    "\n",
    "# Map gold/pred by input text\n",
    "gold_map = {row[\"input\"]: row[\"output\"] for row in gold}\n",
    "pred_map = {row[\"input\"]: row[\"output\"] for row in pred}\n",
    "\n",
    "print(f\"Loaded gold rows: {len(gold_map)}\")\n",
    "print(f\"Loaded pred rows: {len(pred_map)}\")\n",
    "\n",
    "# -----------------------------\n",
    "# EMOTION-ONLY ALIGNMENT\n",
    "# -----------------------------\n",
    "gold_emotions = []\n",
    "pred_emotions = []\n",
    "mismatches = []\n",
    "\n",
    "for text, gold_list in gold_map.items():\n",
    "\n",
    "    pred_list = pred_map.get(text, [])\n",
    "\n",
    "    # Emotion-only invariant:\n",
    "    # - same number of triples\n",
    "    if len(pred_list) != len(gold_list):\n",
    "        mismatches.append({\n",
    "            \"input\": text,\n",
    "            \"gold_count\": len(gold_list),\n",
    "            \"pred_count\": len(pred_list),\n",
    "            \"gold_output\": gold_list,\n",
    "            \"pred_output\": pred_list\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    for gold_triple, pred_triple in zip(gold_list, pred_list):\n",
    "        gold_emotions.append(gold_triple[\"emotion\"])\n",
    "        pred_emotions.append(pred_triple[\"emotion\"])\n",
    "\n",
    "# -----------------------------\n",
    "# SAVE MISMATCHED ROWS\n",
    "# -----------------------------\n",
    "if mismatches:\n",
    "    mismatch_path = f\"emotion_only_mismatches_{input_name}.jsonl\"\n",
    "    with open(mismatch_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for row in mismatches:\n",
    "            f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"\\nâš ï¸  Found {len(mismatches)} mismatched rows\")\n",
    "    print(f\"Saved mismatches â†’ {mismatch_path}\")\n",
    "else:\n",
    "    print(\"\\nâœ… No triple-count mismatches found\")\n",
    "\n",
    "print(f\"Used {len(gold_emotions)} aligned emotion labels for evaluation\")\n",
    "\n",
    "# -----------------------------\n",
    "# Compute Metrics (Emotion Only)\n",
    "# -----------------------------\n",
    "emotion_macro = f1_score(\n",
    "    gold_emotions, pred_emotions,\n",
    "    average=\"macro\", zero_division=0\n",
    ")\n",
    "\n",
    "emotion_micro = f1_score(\n",
    "    gold_emotions, pred_emotions,\n",
    "    average=\"micro\", zero_division=0\n",
    ")\n",
    "\n",
    "correct = sum(1 for g, p in zip(gold_emotions, pred_emotions) if g == p)\n",
    "emotion_accuracy = correct / len(gold_emotions) if gold_emotions else 0.0\n",
    "\n",
    "# -----------------------------\n",
    "# PRINT REPORT\n",
    "# -----------------------------\n",
    "print(\"\\n==============================\")\n",
    "print(\"EMOTION-ONLY EVALUATION REPORT\")\n",
    "print(\"==============================\\n\")\n",
    "\n",
    "print(\"Emotion Macro-F1      :\", round(emotion_macro, 4))\n",
    "print(\"Emotion Micro-F1      :\", round(emotion_micro, 4))\n",
    "print(\"Exact Emotion Accuracy:\", round(emotion_accuracy, 4))\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "\n",
    "print(\"DETAILED EMOTION REPORT\")\n",
    "print(\"\\n==============================\")\n",
    "\n",
    "\n",
    "detailed_classification = classification_report(\n",
    "    gold_emotions,\n",
    "    pred_emotions,\n",
    "    zero_division=0\n",
    ")\n",
    "\n",
    "header = \"emotion           precision    recall  f1-score   support\"\n",
    "detailed_classification = header + \"\\n\" + detailed_classification\n",
    "\n",
    "print(detailed_classification)\n",
    "\n",
    "# -----------------------------\n",
    "# SAVE METRICS CSV\n",
    "# -----------------------------\n",
    "df = pd.DataFrame({\n",
    "    \"Emotion Macro F1\": [emotion_macro],\n",
    "    \"Emotion Micro F1\": [emotion_micro],\n",
    "    \"Emotion Accuracy\": [emotion_accuracy],\n",
    "    \"Detailed Report\": [detailed_classification]\n",
    "})\n",
    "\n",
    "out_csv = f\"f1_metrics_{input_name}.csv\"\n",
    "df.to_csv(out_csv, index=False)\n",
    "\n",
    "print(f\"\\nSaved â†’ {out_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a45cc69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Aspect-Polarity-Emotion evaluation\n",
    "\n",
    "# import json\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# from sklearn.metrics import f1_score, classification_report\n",
    "# from collections import Counter\n",
    "\n",
    "# # -----------------------------\n",
    "# # Paths\n",
    "# # -----------------------------\n",
    "\n",
    "# #GOLD_PATH = os.path.join(data_root, \"daniel_50.jsonl\")\n",
    "# #PRED_PATH = os.path.join(results_root, \"gemini-flash\", \"gemini_annotated_aspect_polarity_emotions_daniel_50.jsonl\")\n",
    "\n",
    "# GOLD_PATH = os.path.join(data_root, \"cleaned_300.jsonl\")\n",
    "# PRED_PATH = os.path.join(results_root, \"gemini-flash\", \"gemini_annotated_aspect_polarity_emotions_300.jsonl\")\n",
    "# # -----------------------------\n",
    "# # Load JSONL\n",
    "# # -----------------------------\n",
    "# def load_jsonl(path):\n",
    "#     return [json.loads(line) for line in open(path, \"r\", encoding=\"utf-8\")]\n",
    "\n",
    "# gold = load_jsonl(GOLD_PATH)\n",
    "# pred = load_jsonl(PRED_PATH)\n",
    "\n",
    "# # Map gold/pred by input text\n",
    "# gold_map = {row[\"input\"]: row[\"output\"] for row in gold}\n",
    "# pred_map = {row[\"input\"]: row[\"output\"] for row in pred}\n",
    "\n",
    "# print(f\"Loaded gold rows: {len(gold_map)}\")\n",
    "# print(f\"Loaded pred rows: {len(pred_map)}\")\n",
    "\n",
    "\n",
    "# # -----------------------------------\n",
    "# # Duplicate detection helper\n",
    "# # -----------------------------------\n",
    "# def find_duplicates(rows, name):\n",
    "#     counter = Counter([row[\"input\"] for row in rows])\n",
    "#     dups = [inp for inp, c in counter.items() if c > 1]\n",
    "\n",
    "#     print(f\"\\n==============================\")\n",
    "#     print(f\"DUPLICATE CHECK â†’ {name}\")\n",
    "#     print(\"==============================\")\n",
    "\n",
    "#     if not dups:\n",
    "#         print(\"No duplicates found.\\n\")\n",
    "#         return {}\n",
    "\n",
    "#     print(f\"Found {len(dups)} duplicate input texts:\")\n",
    "#     for d in dups:\n",
    "#         print(f\" - {d!r}  (x{counter[d]})\")\n",
    "#     print()\n",
    "\n",
    "#     # return mapping of input â†’ list of rows\n",
    "#     dup_map = {d: [r for r in rows if r[\"input\"] == d] for d in dups}\n",
    "#     return dup_map\n",
    "\n",
    "\n",
    "# from collections import defaultdict\n",
    "\n",
    "# def find_duplicate_indices_1_based(rows, name):\n",
    "#     index_map = defaultdict(list)\n",
    "\n",
    "#     for i, row in enumerate(rows):\n",
    "#         index_map[row[\"input\"]].append(i + 1)   # convert to 1-based\n",
    "\n",
    "#     duplicates = {text: idxs for text, idxs in index_map.items() if len(idxs) > 1}\n",
    "\n",
    "#     print(f\"\\n==============================\")\n",
    "#     print(f\"DUPLICATE ROW INDICES â†’ {name}\")\n",
    "#     print(\"==============================\")\n",
    "\n",
    "#     if not duplicates:\n",
    "#         print(\"No duplicates found.\\n\")\n",
    "#         return duplicates\n",
    "\n",
    "#     for text, idxs in duplicates.items():\n",
    "#         print(f\"\\nTEXT: {text!r}\")\n",
    "#         print(f\"1-based row numbers: {idxs}\")\n",
    "\n",
    "#     print()\n",
    "#     return duplicates\n",
    "\n",
    "\n",
    "# gold_dup_indices = find_duplicate_indices_1_based(gold, \"GOLD FILE\")\n",
    "# pred_dup_indices = find_duplicate_indices_1_based(pred, \"PRED FILE\")\n",
    "\n",
    "# # -----------------------------------\n",
    "# # Duplicates appearing in BOTH (with indices)\n",
    "# # -----------------------------------\n",
    "# print(\"\\n==============================\")\n",
    "# print(\"DUPLICATES IN BOTH FILES (WITH ROW NUMBERS)\")\n",
    "# print(\"==============================\")\n",
    "\n",
    "# common = set(gold_dup_indices.keys()) & set(pred_dup_indices.keys())\n",
    "\n",
    "# if not common:\n",
    "#     print(\"No overlapping duplicates.\\n\")\n",
    "# else:\n",
    "#     for text in common:\n",
    "#         print(f\"\\nTEXT: {text!r}\")\n",
    "#         print(f\" â†’ GOLD rows: {gold_dup_indices[text]}\")\n",
    "#         print(f\" â†’ PRED rows: {pred_dup_indices[text]}\")\n",
    "#         print(\"\\n-----------------------------------\")\n",
    "\n",
    "# # -----------------------------\n",
    "# # Triple alignment\n",
    "# # -----------------------------\n",
    "# def align_triples(gold_list, pred_list):\n",
    "#     \"\"\"\n",
    "#     Ensures aligned triples for scoring:\n",
    "#     - If pred has MORE triples â†’ truncate\n",
    "#     - If pred has FEWER triples â†’ pad with ('none','none','none')\n",
    "#     \"\"\"\n",
    "#     g = [(t[\"aspect\"], t[\"polarity\"], t[\"emotion\"]) for t in gold_list]\n",
    "#     p = [(t[\"aspect\"], t[\"polarity\"], t[\"emotion\"]) for t in pred_list]\n",
    "\n",
    "#     gold_n = len(g)\n",
    "#     pred_n = len(p)\n",
    "\n",
    "#     # truncate hallucinations\n",
    "#     if pred_n > gold_n:\n",
    "#         p = p[:gold_n]\n",
    "\n",
    "#     # pad missing predictions\n",
    "#     if pred_n < gold_n:\n",
    "#         pad = [(\"none\", \"none\", \"none\")] * (gold_n - pred_n)\n",
    "#         p.extend(pad)\n",
    "\n",
    "#     return g, p\n",
    "\n",
    "\n",
    "# # -----------------------------\n",
    "# # Collect aligned labels\n",
    "# # -----------------------------\n",
    "# all_gold_as, all_pred_as = [], []\n",
    "# all_gold_pol, all_pred_pol = [], []\n",
    "# all_gold_emo, all_pred_emo = [], []\n",
    "\n",
    "# gold_triples_full = []\n",
    "# pred_triples_full = []\n",
    "\n",
    "# for text, gold_list in gold_map.items():\n",
    "\n",
    "#     pred_list = pred_map.get(text, [])\n",
    "\n",
    "#     g_aligned, p_aligned = align_triples(gold_list, pred_list)\n",
    "\n",
    "#     for (ga, gp, ge), (pa, pp, pe) in zip(g_aligned, p_aligned):\n",
    "#         all_gold_as.append(ga)\n",
    "#         all_gold_pol.append(gp)\n",
    "#         all_gold_emo.append(ge)\n",
    "\n",
    "#         all_pred_as.append(pa)\n",
    "#         all_pred_pol.append(pp)\n",
    "#         all_pred_emo.append(pe)\n",
    "\n",
    "#         gold_triples_full.append((ga, gp, ge))\n",
    "#         pred_triples_full.append((pa, pp, pe))\n",
    "\n",
    "\n",
    "# # -----------------------------\n",
    "# # Compute metrics\n",
    "# # -----------------------------\n",
    "# aspect_f1 = f1_score(all_gold_as, all_pred_as, average=\"macro\", zero_division=0)\n",
    "# polarity_f1 = f1_score(all_gold_pol, all_pred_pol, average=\"macro\", zero_division=0)\n",
    "# emotion_macro = f1_score(all_gold_emo, all_pred_emo, average=\"macro\", zero_division=0)\n",
    "# emotion_micro = f1_score(all_gold_emo, all_pred_emo, average=\"micro\", zero_division=0)\n",
    "\n",
    "# # Full ABSA accuracy (exact triple match)\n",
    "# match = sum(1 for g, p in zip(gold_triples_full, pred_triples_full) if g == p)\n",
    "# exact_acc = match / len(gold_triples_full)\n",
    "\n",
    "\n",
    "# # -----------------------------\n",
    "# # Pretty PRINT\n",
    "# # -----------------------------\n",
    "# print(\"\\n==============================\")\n",
    "# print(\"FINAL EVALUATION REPORT\")\n",
    "# print(\"==============================\\n\")\n",
    "\n",
    "# print(\"ASPECT CLASSIFICATION\")\n",
    "# print(\"------------------------------\")\n",
    "# print(f\"Macro F1: {aspect_f1:.4f}\\n\")\n",
    "\n",
    "# print(\"POLARITY CLASSIFICATION\")\n",
    "# print(\"------------------------------\")\n",
    "# print(f\"Macro F1: {polarity_f1:.4f}\\n\")\n",
    "\n",
    "# print(\"EMOTION CLASSIFICATION\")\n",
    "# print(\"------------------------------\")\n",
    "# print(f\"Emotion Macro-F1 : {emotion_macro:.4f}\")\n",
    "# print(f\"Emotion Micro-F1 : {emotion_micro:.4f}\\n\")\n",
    "\n",
    "# print(\"FULL ABSA TRIPLE MATCH\")\n",
    "# print(\"------------------------------\")\n",
    "# print(f\"Full ABSA Accuracy: {exact_acc:.4f}\\n\")\n",
    "\n",
    "# print(\"==============================\")\n",
    "# print(\"DETAILED CLASSIFICATION REPORT\")\n",
    "# print(\"==============================\\n\")\n",
    "# print(classification_report(all_gold_emo, all_pred_emo, zero_division=0))\n",
    "\n",
    "\n",
    "# # Save optional CSV\n",
    "# df = pd.DataFrame({\n",
    "#     \"Aspect F1\":      [aspect_f1],\n",
    "#     \"Polarity F1\":    [polarity_f1],\n",
    "#     \"Emotion F1\":     [emotion_macro],\n",
    "#     \"Emotion Micro\":  [emotion_micro],\n",
    "#     \"Full ABSA Acc\":  [exact_acc]\n",
    "# })\n",
    "# df.to_csv(f\"eval_results_absa_{GOLD_PATH.split('/')[-1].replace('.jsonl', '')}.csv\", index=False)\n",
    "\n",
    "# print(f\"\\nSaved â†’ eval_results_{GOLD_PATH.split('/')[-1].replace('.jsonl', '')}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
