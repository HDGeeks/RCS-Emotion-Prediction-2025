{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac65636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "PROJECT_MARKERS = (\"src\", \"data\", \"prompts\", \"results\")\n",
    "\n",
    "def find_project_root(start_path):\n",
    "    current = os.path.abspath(start_path)\n",
    "\n",
    "    while True:\n",
    "        if all(os.path.isdir(os.path.join(current, m)) for m in PROJECT_MARKERS):\n",
    "            return current\n",
    "\n",
    "        parent = os.path.dirname(current)\n",
    "        if parent == current:\n",
    "            raise RuntimeError(\"Project root not found\")\n",
    "\n",
    "        current = parent\n",
    "\n",
    "\n",
    "# ---- execution directory (cwd) ----\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# ---- safe starting point ----\n",
    "try:\n",
    "    start_path = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    start_path = cwd\n",
    "\n",
    "\n",
    "# ---- resolve canonical paths ----\n",
    "project_root = find_project_root(start_path)\n",
    "\n",
    "# ‚úÖ THIS IS THE IMPORTANT PART\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "src_root     = os.path.join(project_root, \"src\", \"daniel\", \"gemini\")\n",
    "data_root    = os.path.join(project_root, \"data\", \"MAMS-ACSA\", \"raw\", \"data_jsonl\", \"annotated\")\n",
    "schemas_root = os.path.join(project_root, \"data\", \"MAMS-ACSA\", \"raw\", \"data_jsonl\", \"schema\")\n",
    "prompts_root = os.path.join(project_root, \"prompts\", \"daniel\", \"llama\")\n",
    "utils_root   = os.path.join(project_root, \"utils\")\n",
    "results_root = os.path.join(project_root, \"results\", \"daniel\")\n",
    "\n",
    "print(\n",
    "    f\"üìÇ cwd          : {cwd}\\n\"\n",
    "    f\"üìÇ Project root : {project_root}\\n\"\n",
    "    f\"üìÇ Source root  : {src_root}\\n\"\n",
    "    f\"üìÇ Data root    : {data_root}\\n\"\n",
    "    f\"üìÇ Prompts root : {prompts_root}\\n\"\n",
    "    f\"üìÇ Utils root   : {utils_root}\\n\"\n",
    "    f\"üìÇ Results root : {results_root}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb845b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALIDATION BLOCK\n",
    "\n",
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "# -----------------------------\n",
    "# Paths\n",
    "# -----------------------------\n",
    "GOLD_PATH = os.path.join(data_root, \"02_iteration_cleaned_300.jsonl\")\n",
    "#PRED_PATH = os.path.join(results_root, \"llama\", \"llama_output_updated.jsonl\")\n",
    "PRED_PATH = os.path.join(results_root, \"gpt\", \"gpt_output_v2_emotion_only.jsonl\")\n",
    "\n",
    "input_name = os.path.splitext(os.path.basename(PRED_PATH))[0]\n",
    "AUDIT_PATH = os.path.join(\n",
    "    results_root, f\"audit_invalid_rows_{input_name}.jsonl\"\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Load JSONL\n",
    "# -----------------------------\n",
    "def load_jsonl(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "gold = load_jsonl(GOLD_PATH)\n",
    "pred = load_jsonl(PRED_PATH)\n",
    "\n",
    "assert len(gold) == len(pred), \"Row count mismatch!\"\n",
    "\n",
    "# -----------------------------\n",
    "# Validation\n",
    "# -----------------------------\n",
    "invalid_rows = []\n",
    "valid_indices = []\n",
    "\n",
    "for idx, (g, p) in enumerate(zip(gold, pred)):\n",
    "\n",
    "    # Rule 1: input must match\n",
    "    if g[\"input\"] != p[\"input\"]:\n",
    "        invalid_rows.append({\n",
    "            \"index\": idx,\n",
    "            \"reason\": \"input_mismatch\",\n",
    "            \"gold_input\": g[\"input\"],\n",
    "            \"pred_input\": p[\"input\"]\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    g_out = g[\"output\"]\n",
    "    p_out = p[\"output\"]\n",
    "\n",
    "    # Rule 2: same number of triples\n",
    "    if len(g_out) != len(p_out):\n",
    "        invalid_rows.append({\n",
    "            \"index\": idx,\n",
    "            \"reason\": \"output_count_mismatch\",\n",
    "            \"input\": g[\"input\"],\n",
    "            \"gold_count\": len(g_out),\n",
    "            \"pred_count\": len(p_out),\n",
    "            \"gold_output\": g_out,\n",
    "            \"pred_output\": p_out\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    # Rule 3: same (aspect, polarity)\n",
    "    g_pairs = Counter((o[\"aspect\"], o[\"polarity\"]) for o in g_out)\n",
    "    p_pairs = Counter((o[\"aspect\"], o[\"polarity\"]) for o in p_out)\n",
    "\n",
    "    if g_pairs != p_pairs:\n",
    "        invalid_rows.append({\n",
    "            \"index\": idx,\n",
    "            \"reason\": \"aspect_polarity_mismatch\",\n",
    "            \"input\": g[\"input\"],\n",
    "            \"gold_aspect_polarity\": [\n",
    "                {\"aspect\": a, \"polarity\": p, \"count\": c}\n",
    "                for (a, p), c in g_pairs.items()\n",
    "            ],\n",
    "            \"pred_aspect_polarity\": [\n",
    "                {\"aspect\": a, \"polarity\": p, \"count\": c}\n",
    "                for (a, p), c in p_pairs.items()\n",
    "            ],\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    # Row is valid\n",
    "    valid_indices.append(idx)\n",
    "\n",
    "# -----------------------------\n",
    "# Report + save\n",
    "# -----------------------------\n",
    "print(\"VALIDATION SUMMARY\")\n",
    "print(\"------------------\")\n",
    "print(f\"Total rows : {len(gold)}\")\n",
    "print(f\"Valid rows : {len(valid_indices)}\")\n",
    "print(f\"Invalid    : {len(invalid_rows)}\")\n",
    "\n",
    "if invalid_rows:\n",
    "    with open(AUDIT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        for r in invalid_rows:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"\\nInvalid rows written to:\")\n",
    "    print(AUDIT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a1ec3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "gold_emotions = []\n",
    "pred_emotions = []\n",
    "\n",
    "for idx in valid_indices:\n",
    "    g_out = gold[idx][\"output\"]\n",
    "    p_out = pred[idx][\"output\"]\n",
    "\n",
    "    # Safe: structure already validated\n",
    "    for g, p in zip(g_out, p_out):\n",
    "        gold_emotions.append(g[\"emotion\"])\n",
    "        pred_emotions.append(p[\"emotion\"])\n",
    "\n",
    "print(f\"Used {len(gold_emotions)} emotion labels for evaluation\")\n",
    "\n",
    "emotion_macro = f1_score(\n",
    "    gold_emotions, pred_emotions,\n",
    "    average=\"macro\", zero_division=0\n",
    ")\n",
    "\n",
    "emotion_micro = f1_score(\n",
    "    gold_emotions, pred_emotions,\n",
    "    average=\"micro\", zero_division=0\n",
    ")\n",
    "\n",
    "emotion_accuracy = sum(\n",
    "    g == p for g, p in zip(gold_emotions, pred_emotions)\n",
    ") / len(gold_emotions)\n",
    "\n",
    "print(\"\\nEMOTION-ONLY EVALUATION REPORT\")\n",
    "print(\"--------------------------------\")\n",
    "print(\"Emotion Macro-F1      :\", round(emotion_macro, 4))\n",
    "print(\"Emotion Micro-F1      :\", round(emotion_micro, 4))\n",
    "print(\"Exact Emotion Accuracy:\", round(emotion_accuracy, 4))\n",
    "\n",
    "print(\"\\nDETAILED EMOTION REPORT\\n\")\n",
    "\n",
    "detailed_report = classification_report(\n",
    "    gold_emotions,\n",
    "    pred_emotions,\n",
    "    zero_division=0\n",
    ")\n",
    "\n",
    "print(detailed_report)\n",
    "EMOTION_ABBR = {\n",
    "    \"admiration\":       \"ADM\",\n",
    "    \"annoyance\":        \"ANN\",\n",
    "    \"disappointment\":  \"DIS\",\n",
    "    \"disgust\":          \"DGS\",\n",
    "    \"gratitude\":        \"GRT\",\n",
    "    \"mentioned_only\":   \"MNT\",\n",
    "    \"mixed_emotions\":   \"MIX\",\n",
    "    \"no_emotion\":       \"NON\",\n",
    "    \"satisfaction\":    \"SAT\",\n",
    "}\n",
    "# -----------------------------\n",
    "# CONFUSION MATRIX (Emotion)\n",
    "# -----------------------------\n",
    "\n",
    "labels = sorted(set(gold_emotions))\n",
    "\n",
    "cm = confusion_matrix(\n",
    "    gold_emotions,\n",
    "    pred_emotions,\n",
    "    labels=labels\n",
    ")\n",
    "\n",
    "cm_df = pd.DataFrame(\n",
    "    cm,\n",
    "    index=[f\"G_{EMOTION_ABBR[l]}\" for l in labels],\n",
    "    columns=[f\"P_{EMOTION_ABBR[l]}\" for l in labels]\n",
    ")\n",
    "\n",
    "print(\"\\nCONFUSION MATRIX (COUNTS ‚Äî COMPACT)\\n\")\n",
    "print(cm_df)\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "\n",
    "cm_norm = confusion_matrix(\n",
    "    gold_emotions,\n",
    "    pred_emotions,\n",
    "    labels=labels,\n",
    "    normalize=\"true\"\n",
    ")\n",
    "\n",
    "cm_norm_df = pd.DataFrame(\n",
    "    cm_norm,\n",
    "    index=[f\"G_{EMOTION_ABBR[l]}\" for l in labels],\n",
    "    columns=[f\"P_{EMOTION_ABBR[l]}\" for l in labels]\n",
    ")\n",
    "\n",
    "print(\"\\nCONFUSION MATRIX (NORMALIZED ‚Äî COMPACT)\\n\")\n",
    "print(cm_norm_df.round(2))\n",
    "\n",
    "\n",
    "cm_out = f\"confusion_matrix_emotion_compact_{input_name}.csv\"\n",
    "cm_norm_out = f\"confusion_matrix_emotion_compact_norm_{input_name}.csv\"\n",
    "\n",
    "cm_df.to_csv(cm_out)\n",
    "cm_norm_df.to_csv(cm_norm_out)\n",
    "\n",
    "print(\"\\nSaved confusion matrices:\")\n",
    "print(cm_out)\n",
    "print(cm_norm_out)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "ax = sns.heatmap(\n",
    "    cm_df,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Oranges\",\n",
    "    linewidths=0.5,\n",
    "    linecolor=\"gray\",\n",
    "    cbar_kws={\"label\": \"Count\"},\n",
    "    square=True\n",
    ")\n",
    "\n",
    "# --- move column labels to TOP ---\n",
    "ax.xaxis.tick_top()\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "\n",
    "# rotate for readability\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"left\")\n",
    "plt.setp(ax.get_yticklabels(), rotation=0)\n",
    "\n",
    "# emphasize diagonal\n",
    "for i in range(len(cm_df)):\n",
    "    ax.add_patch(\n",
    "        plt.Rectangle(\n",
    "            (i, i), 1, 1,\n",
    "            fill=False,\n",
    "            edgecolor=\"black\",\n",
    "            lw=2\n",
    "        )\n",
    "    )\n",
    "\n",
    "plt.title(\"Emotion Confusion Matrix (Counts)\", fontsize=14, pad=30)\n",
    "plt.xlabel(\"Predicted Emotion\", fontsize=12)\n",
    "plt.ylabel(\"Gold Emotion\", fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# save as PNG\n",
    "png_path = f\"confusion_matrix_emotion_counts_{input_name}.png\"\n",
    "plt.savefig(png_path, dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Saved PNG ‚Üí {png_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9130da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ============================================================\n",
    "# # ASPECT + POLARITY + EMOTION EVALUATION (STRICT ORDER MATCH)\n",
    "# # ============================================================\n",
    "# #\n",
    "# # What this script DOES:\n",
    "# # - Aligns gold/pred by exact \"input\" text (sentence-level alignment)\n",
    "# # - Enforces: same number of triplets per sentence (count match)\n",
    "# # - Enforces: same order by position (triplet i in gold compared to triplet i in pred)\n",
    "# # - Evaluates THREE label types separately:\n",
    "# #     1) aspect\n",
    "# #     2) polarity\n",
    "# #     3) emotion\n",
    "# #\n",
    "# # What this script DOES NOT do:\n",
    "# # - No re-ordering, no fuzzy matching, no deduplication\n",
    "# # - If the model changes triplet count for a sentence ‚Üí that sentence is excluded & logged\n",
    "# #\n",
    "# # Why this is correct for your setup:\n",
    "# # - Your dataset defines the \"structure\" (triplets and their order).\n",
    "# # - Models are allowed to predict labels; we evaluate them only when structure is comparable.\n",
    "# # ============================================================\n",
    "\n",
    "# import json\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# # -----------------------------\n",
    "# # Paths\n",
    "# # -----------------------------\n",
    "# GOLD_PATH = os.path.join(data_root, \"02_iteration_cleaned_300.jsonl\")\n",
    "\n",
    "# # Choose exactly ONE prediction file\n",
    "# #PRED_PATH = os.path.join(results_root, \"llama\", \"llama_output.jsonl\")\n",
    "# #PRED_PATH = os.path.join(results_root, \"claude\", \"claude_output.jsonl\")\n",
    "# # PRED_PATH = os.path.join(results_root, \"gpt\", \"gpt_output.jsonl\")\n",
    "# #PRED_PATH = os.path.join(results_root, \"deepseek\", \"deep_seek_output.jsonl\")\n",
    "# PRED_PATH = os.path.join(results_root, \"llama\", \"llama_output_updated.jsonl\")\n",
    "\n",
    "# # Tag outputs with model/file name\n",
    "# input_name = os.path.splitext(os.path.basename(PRED_PATH))[0]\n",
    "\n",
    "# # -----------------------------\n",
    "# # Load JSONL\n",
    "# # -----------------------------\n",
    "# def load_jsonl(path: str):\n",
    "#     \"\"\"Load a JSONL file into a list of dicts (one JSON object per line).\"\"\"\n",
    "#     with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         return [json.loads(line) for line in f]\n",
    "\n",
    "# gold = load_jsonl(GOLD_PATH)\n",
    "# pred = load_jsonl(PRED_PATH)\n",
    "\n",
    "# # -----------------------------\n",
    "# # Align by input text\n",
    "# # -----------------------------\n",
    "# # NOTE: If duplicate \"input\" texts exist in a file, dict will keep only the LAST one.\n",
    "# gold_map = {row[\"input\"]: row[\"output\"] for row in gold}\n",
    "# pred_map = {row[\"input\"]: row[\"output\"] for row in pred}\n",
    "\n",
    "# print(f\"Loaded gold rows: {len(gold_map)}\")\n",
    "# print(f\"Loaded pred rows: {len(pred_map)}\")\n",
    "\n",
    "# # -----------------------------\n",
    "# # Collect aligned labels\n",
    "# # -----------------------------\n",
    "# gold_aspects, pred_aspects = [], []\n",
    "# gold_polarities, pred_polarities = [], []\n",
    "# gold_emotions, pred_emotions = [], []\n",
    "\n",
    "# mismatches = []\n",
    "\n",
    "# for text, gold_list in gold_map.items():\n",
    "#     pred_list = pred_map.get(text)\n",
    "\n",
    "#     # Case 1: sentence missing in prediction file\n",
    "#     if pred_list is None:\n",
    "#         mismatches.append({\n",
    "#             \"input\": text,\n",
    "#             \"reason\": \"MISSING_PREDICTION\",\n",
    "#             \"gold_count\": len(gold_list),\n",
    "#             \"pred_count\": 0,\n",
    "#             \"gold_output\": gold_list,\n",
    "#             \"pred_output\": []\n",
    "#         })\n",
    "#         continue\n",
    "\n",
    "#     # Case 2: triplet count mismatch (structure not comparable)\n",
    "#     if len(pred_list) != len(gold_list):\n",
    "#         mismatches.append({\n",
    "#             \"input\": text,\n",
    "#             \"reason\": \"TRIPLE_COUNT_MISMATCH\",\n",
    "#             \"gold_count\": len(gold_list),\n",
    "#             \"pred_count\": len(pred_list),\n",
    "#             \"gold_output\": gold_list,\n",
    "#             \"pred_output\": pred_list\n",
    "#         })\n",
    "#         continue\n",
    "\n",
    "#     # Case 3: strict order-by-position comparison\n",
    "#     # We compare triplet i with triplet i. No re-matching.\n",
    "#     for idx, (g, p) in enumerate(zip(gold_list, pred_list)):\n",
    "#         # Extract labels from gold (always present if gold is clean)\n",
    "#         g_aspect = g.get(\"aspect\")\n",
    "#         g_polarity = g.get(\"polarity\")\n",
    "#         g_emotion = g.get(\"emotion\")\n",
    "\n",
    "#         # Extract labels from pred (use .get to avoid KeyError; missing becomes None)\n",
    "#         p_aspect = p.get(\"aspect\")\n",
    "#         p_polarity = p.get(\"polarity\")\n",
    "#         p_emotion = p.get(\"emotion\")\n",
    "\n",
    "#         # If a model forgets a field entirely, log it as a structural issue\n",
    "#         # (because you cannot evaluate a missing label fairly).\n",
    "#         if p_aspect is None or p_polarity is None or p_emotion is None:\n",
    "#             mismatches.append({\n",
    "#                 \"input\": text,\n",
    "#                 \"reason\": \"MISSING_FIELDS_IN_TRIPLE\",\n",
    "#                 \"index\": idx,\n",
    "#                 \"gold_triple\": g,\n",
    "#                 \"pred_triple\": p\n",
    "#             })\n",
    "#             # Discard the whole sentence to keep evaluation strict and comparable\n",
    "#             break\n",
    "\n",
    "#         # Append aligned labels\n",
    "#         gold_aspects.append(g_aspect);     pred_aspects.append(p_aspect)\n",
    "#         gold_polarities.append(g_polarity); pred_polarities.append(p_polarity)\n",
    "#         gold_emotions.append(g_emotion);   pred_emotions.append(p_emotion)\n",
    "#     else:\n",
    "#         # The for-loop didn't break -> sentence was fully processed\n",
    "#         continue\n",
    "\n",
    "#     # If we broke due to missing fields, we should remove any partially appended labels\n",
    "#     # for this sentence. Easiest strict approach: avoid partial contamination by rewinding.\n",
    "#     # We know exactly how many triplets were in the sentence: len(gold_list)\n",
    "#     # BUT we may have broken early at idx, so we must remove (idx already appended?) careful:\n",
    "#     # In this code, we append only AFTER missing-field check, so if we break, nothing for that idx\n",
    "#     # but earlier triplets of this sentence WERE appended. We must remove those.\n",
    "#     k = 0\n",
    "#     # Count how many from this sentence were appended before break:\n",
    "#     # that's exactly the number of iterations completed before break,\n",
    "#     # which equals the current idx value at break time.\n",
    "#     # However idx is out of scope here, so we recompute based on the mismatch record.\n",
    "#     # Minimal safe approach: remove len(gold_list) labels ONLY if we processed the full sentence.\n",
    "#     # Since we broke, we need to remove the already-added part:\n",
    "#     last = mismatches[-1]\n",
    "#     if last.get(\"reason\") == \"MISSING_FIELDS_IN_TRIPLE\" and last.get(\"input\") == text:\n",
    "#         k = last.get(\"index\", 0)\n",
    "#         if k > 0:\n",
    "#             del gold_aspects[-k:]; del pred_aspects[-k:]\n",
    "#             del gold_polarities[-k:]; del pred_polarities[-k:]\n",
    "#             del gold_emotions[-k:]; del pred_emotions[-k:]\n",
    "\n",
    "# # -----------------------------\n",
    "# # Save mismatches\n",
    "# # -----------------------------\n",
    "# if mismatches:\n",
    "#     mismatch_path = f\"full_task_mismatches_{input_name}.jsonl\"\n",
    "#     with open(mismatch_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#         for row in mismatches:\n",
    "#             f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "#     print(f\"\\n‚ö†Ô∏è  Found {len(mismatches)} invalid sentences\")\n",
    "#     print(f\"Saved mismatches ‚Üí {mismatch_path}\")\n",
    "# else:\n",
    "#     print(\"\\n‚úÖ No mismatches found\")\n",
    "\n",
    "# print(f\"Used {len(gold_emotions)} aligned triplets for evaluation (3 labels each)\")\n",
    "\n",
    "# # -----------------------------\n",
    "# # Metric helper\n",
    "# # -----------------------------\n",
    "# def eval_labels(gold_labels, pred_labels, name: str):\n",
    "#     \"\"\"Compute macro/micro F1 + accuracy and generate a detailed classification report.\"\"\"\n",
    "#     macro = f1_score(gold_labels, pred_labels, average=\"macro\", zero_division=0)\n",
    "#     micro = f1_score(gold_labels, pred_labels, average=\"micro\", zero_division=0)\n",
    "#     acc = sum(g == p for g, p in zip(gold_labels, pred_labels)) / len(gold_labels) if gold_labels else 0.0\n",
    "\n",
    "#     report = classification_report(gold_labels, pred_labels, zero_division=0)\n",
    "#     # Add a readable header without duplicating sklearn's header:\n",
    "#     # We only prepend a label column name line; sklearn prints precision/recall/f1/support itself.\n",
    "#     header = f\"{name:<16}precision    recall  f1-score   support\"\n",
    "#     report = header + \"\\n\" + report\n",
    "\n",
    "#     return macro, micro, acc, report\n",
    "\n",
    "# # -----------------------------\n",
    "# # Compute metrics for each label type\n",
    "# # -----------------------------\n",
    "# aspect_macro, aspect_micro, aspect_acc, aspect_report = eval_labels(gold_aspects, pred_aspects, \"aspect\")\n",
    "# pol_macro, pol_micro, pol_acc, pol_report = eval_labels(gold_polarities, pred_polarities, \"polarity\")\n",
    "# emo_macro, emo_micro, emo_acc, emo_report = eval_labels(gold_emotions, pred_emotions, \"emotion\")\n",
    "\n",
    "# # -----------------------------\n",
    "# # Print summary\n",
    "# # -----------------------------\n",
    "# print(\"\\n==============================\")\n",
    "# print(\"FULL TASK EVALUATION SUMMARY\")\n",
    "# print(\"==============================\\n\")\n",
    "\n",
    "# print(\"ASPECT   - Macro-F1:\", round(aspect_macro, 4), \" Micro-F1:\", round(aspect_micro, 4), \" Acc:\", round(aspect_acc, 4))\n",
    "# print(\"POLARITY - Macro-F1:\", round(pol_macro, 4),    \" Micro-F1:\", round(pol_micro, 4),    \" Acc:\", round(pol_acc, 4))\n",
    "# print(\"EMOTION  - Macro-F1:\", round(emo_macro, 4),    \" Micro-F1:\", round(emo_micro, 4),    \" Acc:\", round(emo_acc, 4))\n",
    "\n",
    "# print(\"\\n==============================\")\n",
    "# print(\"DETAILED REPORTS\")\n",
    "# print(\"==============================\\n\")\n",
    "\n",
    "# print(\"\\n--- ASPECT REPORT ---\\n\")\n",
    "# print(aspect_report)\n",
    "\n",
    "# print(\"\\n--- POLARITY REPORT ---\\n\")\n",
    "# print(pol_report)\n",
    "\n",
    "# print(\"\\n--- EMOTION REPORT ---\\n\")\n",
    "# print(emo_report)\n",
    "\n",
    "# # -----------------------------\n",
    "# # Save metrics in ONE CSV\n",
    "# # -----------------------------\n",
    "# df = pd.DataFrame({\n",
    "#     \"Aspect Macro F1\": [aspect_macro],\n",
    "#     \"Aspect Micro F1\": [aspect_micro],\n",
    "#     \"Aspect Accuracy\": [aspect_acc],\n",
    "\n",
    "#     \"Polarity Macro F1\": [pol_macro],\n",
    "#     \"Polarity Micro F1\": [pol_micro],\n",
    "#     \"Polarity Accuracy\": [pol_acc],\n",
    "\n",
    "#     \"Emotion Macro F1\": [emo_macro],\n",
    "#     \"Emotion Micro F1\": [emo_micro],\n",
    "#     \"Emotion Accuracy\": [emo_acc],\n",
    "\n",
    "#     \"Aspect Report\": [aspect_report],\n",
    "#     \"Polarity Report\": [pol_report],\n",
    "#     \"Emotion Report\": [emo_report],\n",
    "# })\n",
    "\n",
    "# out_csv = f\"f1_metrics_full_{input_name}.csv\"\n",
    "# df.to_csv(out_csv, index=False)\n",
    "\n",
    "# print(f\"\\nSaved ‚Üí {out_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8741d5f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45cc69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Aspect-Polarity-Emotion evaluation\n",
    "\n",
    "# import json\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# from sklearn.metrics import f1_score, classification_report\n",
    "# from collections import Counter\n",
    "\n",
    "# # -----------------------------\n",
    "# # Paths\n",
    "# # -----------------------------\n",
    "\n",
    "# #GOLD_PATH = os.path.join(data_root, \"daniel_50.jsonl\")\n",
    "# #PRED_PATH = os.path.join(results_root, \"gemini-flash\", \"gemini_annotated_aspect_polarity_emotions_daniel_50.jsonl\")\n",
    "\n",
    "# GOLD_PATH = os.path.join(data_root, \"cleaned_300.jsonl\")\n",
    "# PRED_PATH = os.path.join(results_root, \"gemini-flash\", \"gemini_annotated_aspect_polarity_emotions_300.jsonl\")\n",
    "# # -----------------------------\n",
    "# # Load JSONL\n",
    "# # -----------------------------\n",
    "# def load_jsonl(path):\n",
    "#     return [json.loads(line) for line in open(path, \"r\", encoding=\"utf-8\")]\n",
    "\n",
    "# gold = load_jsonl(GOLD_PATH)\n",
    "# pred = load_jsonl(PRED_PATH)\n",
    "\n",
    "# # Map gold/pred by input text\n",
    "# gold_map = {row[\"input\"]: row[\"output\"] for row in gold}\n",
    "# pred_map = {row[\"input\"]: row[\"output\"] for row in pred}\n",
    "\n",
    "# print(f\"Loaded gold rows: {len(gold_map)}\")\n",
    "# print(f\"Loaded pred rows: {len(pred_map)}\")\n",
    "\n",
    "\n",
    "# # -----------------------------------\n",
    "# # Duplicate detection helper\n",
    "# # -----------------------------------\n",
    "# def find_duplicates(rows, name):\n",
    "#     counter = Counter([row[\"input\"] for row in rows])\n",
    "#     dups = [inp for inp, c in counter.items() if c > 1]\n",
    "\n",
    "#     print(f\"\\n==============================\")\n",
    "#     print(f\"DUPLICATE CHECK ‚Üí {name}\")\n",
    "#     print(\"==============================\")\n",
    "\n",
    "#     if not dups:\n",
    "#         print(\"No duplicates found.\\n\")\n",
    "#         return {}\n",
    "\n",
    "#     print(f\"Found {len(dups)} duplicate input texts:\")\n",
    "#     for d in dups:\n",
    "#         print(f\" - {d!r}  (x{counter[d]})\")\n",
    "#     print()\n",
    "\n",
    "#     # return mapping of input ‚Üí list of rows\n",
    "#     dup_map = {d: [r for r in rows if r[\"input\"] == d] for d in dups}\n",
    "#     return dup_map\n",
    "\n",
    "\n",
    "# from collections import defaultdict\n",
    "\n",
    "# def find_duplicate_indices_1_based(rows, name):\n",
    "#     index_map = defaultdict(list)\n",
    "\n",
    "#     for i, row in enumerate(rows):\n",
    "#         index_map[row[\"input\"]].append(i + 1)   # convert to 1-based\n",
    "\n",
    "#     duplicates = {text: idxs for text, idxs in index_map.items() if len(idxs) > 1}\n",
    "\n",
    "#     print(f\"\\n==============================\")\n",
    "#     print(f\"DUPLICATE ROW INDICES ‚Üí {name}\")\n",
    "#     print(\"==============================\")\n",
    "\n",
    "#     if not duplicates:\n",
    "#         print(\"No duplicates found.\\n\")\n",
    "#         return duplicates\n",
    "\n",
    "#     for text, idxs in duplicates.items():\n",
    "#         print(f\"\\nTEXT: {text!r}\")\n",
    "#         print(f\"1-based row numbers: {idxs}\")\n",
    "\n",
    "#     print()\n",
    "#     return duplicates\n",
    "\n",
    "\n",
    "# gold_dup_indices = find_duplicate_indices_1_based(gold, \"GOLD FILE\")\n",
    "# pred_dup_indices = find_duplicate_indices_1_based(pred, \"PRED FILE\")\n",
    "\n",
    "# # -----------------------------------\n",
    "# # Duplicates appearing in BOTH (with indices)\n",
    "# # -----------------------------------\n",
    "# print(\"\\n==============================\")\n",
    "# print(\"DUPLICATES IN BOTH FILES (WITH ROW NUMBERS)\")\n",
    "# print(\"==============================\")\n",
    "\n",
    "# common = set(gold_dup_indices.keys()) & set(pred_dup_indices.keys())\n",
    "\n",
    "# if not common:\n",
    "#     print(\"No overlapping duplicates.\\n\")\n",
    "# else:\n",
    "#     for text in common:\n",
    "#         print(f\"\\nTEXT: {text!r}\")\n",
    "#         print(f\" ‚Üí GOLD rows: {gold_dup_indices[text]}\")\n",
    "#         print(f\" ‚Üí PRED rows: {pred_dup_indices[text]}\")\n",
    "#         print(\"\\n-----------------------------------\")\n",
    "\n",
    "# # -----------------------------\n",
    "# # Triple alignment\n",
    "# # -----------------------------\n",
    "# def align_triples(gold_list, pred_list):\n",
    "#     \"\"\"\n",
    "#     Ensures aligned triples for scoring:\n",
    "#     - If pred has MORE triples ‚Üí truncate\n",
    "#     - If pred has FEWER triples ‚Üí pad with ('none','none','none')\n",
    "#     \"\"\"\n",
    "#     g = [(t[\"aspect\"], t[\"polarity\"], t[\"emotion\"]) for t in gold_list]\n",
    "#     p = [(t[\"aspect\"], t[\"polarity\"], t[\"emotion\"]) for t in pred_list]\n",
    "\n",
    "#     gold_n = len(g)\n",
    "#     pred_n = len(p)\n",
    "\n",
    "#     # truncate hallucinations\n",
    "#     if pred_n > gold_n:\n",
    "#         p = p[:gold_n]\n",
    "\n",
    "#     # pad missing predictions\n",
    "#     if pred_n < gold_n:\n",
    "#         pad = [(\"none\", \"none\", \"none\")] * (gold_n - pred_n)\n",
    "#         p.extend(pad)\n",
    "\n",
    "#     return g, p\n",
    "\n",
    "\n",
    "# # -----------------------------\n",
    "# # Collect aligned labels\n",
    "# # -----------------------------\n",
    "# all_gold_as, all_pred_as = [], []\n",
    "# all_gold_pol, all_pred_pol = [], []\n",
    "# all_gold_emo, all_pred_emo = [], []\n",
    "\n",
    "# gold_triples_full = []\n",
    "# pred_triples_full = []\n",
    "\n",
    "# for text, gold_list in gold_map.items():\n",
    "\n",
    "#     pred_list = pred_map.get(text, [])\n",
    "\n",
    "#     g_aligned, p_aligned = align_triples(gold_list, pred_list)\n",
    "\n",
    "#     for (ga, gp, ge), (pa, pp, pe) in zip(g_aligned, p_aligned):\n",
    "#         all_gold_as.append(ga)\n",
    "#         all_gold_pol.append(gp)\n",
    "#         all_gold_emo.append(ge)\n",
    "\n",
    "#         all_pred_as.append(pa)\n",
    "#         all_pred_pol.append(pp)\n",
    "#         all_pred_emo.append(pe)\n",
    "\n",
    "#         gold_triples_full.append((ga, gp, ge))\n",
    "#         pred_triples_full.append((pa, pp, pe))\n",
    "\n",
    "\n",
    "# # -----------------------------\n",
    "# # Compute metrics\n",
    "# # -----------------------------\n",
    "# aspect_f1 = f1_score(all_gold_as, all_pred_as, average=\"macro\", zero_division=0)\n",
    "# polarity_f1 = f1_score(all_gold_pol, all_pred_pol, average=\"macro\", zero_division=0)\n",
    "# emotion_macro = f1_score(all_gold_emo, all_pred_emo, average=\"macro\", zero_division=0)\n",
    "# emotion_micro = f1_score(all_gold_emo, all_pred_emo, average=\"micro\", zero_division=0)\n",
    "\n",
    "# # Full ABSA accuracy (exact triple match)\n",
    "# match = sum(1 for g, p in zip(gold_triples_full, pred_triples_full) if g == p)\n",
    "# exact_acc = match / len(gold_triples_full)\n",
    "\n",
    "\n",
    "# # -----------------------------\n",
    "# # Pretty PRINT\n",
    "# # -----------------------------\n",
    "# print(\"\\n==============================\")\n",
    "# print(\"FINAL EVALUATION REPORT\")\n",
    "# print(\"==============================\\n\")\n",
    "\n",
    "# print(\"ASPECT CLASSIFICATION\")\n",
    "# print(\"------------------------------\")\n",
    "# print(f\"Macro F1: {aspect_f1:.4f}\\n\")\n",
    "\n",
    "# print(\"POLARITY CLASSIFICATION\")\n",
    "# print(\"------------------------------\")\n",
    "# print(f\"Macro F1: {polarity_f1:.4f}\\n\")\n",
    "\n",
    "# print(\"EMOTION CLASSIFICATION\")\n",
    "# print(\"------------------------------\")\n",
    "# print(f\"Emotion Macro-F1 : {emotion_macro:.4f}\")\n",
    "# print(f\"Emotion Micro-F1 : {emotion_micro:.4f}\\n\")\n",
    "\n",
    "# print(\"FULL ABSA TRIPLE MATCH\")\n",
    "# print(\"------------------------------\")\n",
    "# print(f\"Full ABSA Accuracy: {exact_acc:.4f}\\n\")\n",
    "\n",
    "# print(\"==============================\")\n",
    "# print(\"DETAILED CLASSIFICATION REPORT\")\n",
    "# print(\"==============================\\n\")\n",
    "# print(classification_report(all_gold_emo, all_pred_emo, zero_division=0))\n",
    "\n",
    "\n",
    "# # Save optional CSV\n",
    "# df = pd.DataFrame({\n",
    "#     \"Aspect F1\":      [aspect_f1],\n",
    "#     \"Polarity F1\":    [polarity_f1],\n",
    "#     \"Emotion F1\":     [emotion_macro],\n",
    "#     \"Emotion Micro\":  [emotion_micro],\n",
    "#     \"Full ABSA Acc\":  [exact_acc]\n",
    "# })\n",
    "# df.to_csv(f\"eval_results_absa_{GOLD_PATH.split('/')[-1].replace('.jsonl', '')}.csv\", index=False)\n",
    "\n",
    "# print(f\"\\nSaved ‚Üí eval_results_{GOLD_PATH.split('/')[-1].replace('.jsonl', '')}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
