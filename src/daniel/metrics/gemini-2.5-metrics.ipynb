{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac65636b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Project root: /Users/hd/Desktop/EMOTION-PRED\n",
      "ðŸ“‚ Source root: /Users/hd/Desktop/EMOTION-PRED/src\n",
      "ðŸ“‚ Results root: /Users/hd/Desktop/EMOTION-PRED/src/results\n",
      "ðŸ“‚ Data root: /Users/hd/Desktop/EMOTION-PRED/src/data/MAMS-ACSA/raw/data_jsonl\n",
      "Using dataset directory: /Users/hd/Desktop/EMOTION-PRED/src/data/MAMS-ACSA/raw/data_jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "PROJECT_MARKERS = (\"src\", \"data\", \"prompts\", \"results\")\n",
    "\n",
    "def find_project_root(start_path):\n",
    "    current = os.path.abspath(start_path)\n",
    "\n",
    "    while True:\n",
    "        if all(os.path.isdir(os.path.join(current, m)) for m in PROJECT_MARKERS):\n",
    "            return current\n",
    "\n",
    "        parent = os.path.dirname(current)\n",
    "        if parent == current:\n",
    "            raise RuntimeError(\"Project root not found\")\n",
    "\n",
    "        current = parent\n",
    "\n",
    "\n",
    "# ---- execution directory (cwd) ----\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# ---- safe starting point ----\n",
    "try:\n",
    "    start_path = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    start_path = cwd\n",
    "\n",
    "\n",
    "# ---- resolve canonical paths ----\n",
    "project_root = find_project_root(start_path)\n",
    "\n",
    "# âœ… THIS IS THE IMPORTANT PART\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "src_root     = os.path.join(project_root, \"src\", \"daniel\", \"gemini\")\n",
    "data_root    = os.path.join(project_root, \"data\", \"MAMS-ACSA\", \"raw\", \"data_jsonl\", \"annotated\")\n",
    "schemas_root = os.path.join(project_root, \"data\", \"MAMS-ACSA\", \"raw\", \"data_jsonl\", \"schema\")\n",
    "prompts_root = os.path.join(project_root, \"prompts\", \"daniel\", \"gemini\")\n",
    "utils_root   = os.path.join(project_root, \"utils\")\n",
    "results_root = os.path.join(project_root, \"results\", \"daniel\", \"gemini\")\n",
    "\n",
    "print(\n",
    "    f\"ðŸ“‚ cwd          : {cwd}\\n\"\n",
    "    f\"ðŸ“‚ Project root : {project_root}\\n\"\n",
    "    f\"ðŸ“‚ Source root  : {src_root}\\n\"\n",
    "    f\"ðŸ“‚ Data root    : {data_root}\\n\"\n",
    "    f\"ðŸ“‚ Prompts root : {prompts_root}\\n\"\n",
    "    f\"ðŸ“‚ Utils root   : {utils_root}\\n\"\n",
    "    f\"ðŸ“‚ Results root : {results_root}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8741d5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded gold rows: 300\n",
      "Loaded pred rows: 300\n",
      "\n",
      "==============================\n",
      "EMOTION-ONLY EVALUATION REPORT\n",
      "==============================\n",
      "\n",
      "Emotion Macro-F1 : 0.3731\n",
      "Emotion Micro-F1 : 0.5799\n",
      "Exact Emotion Accuracy: 0.5799\n",
      "\n",
      "==============================\n",
      "DETAILED EMOTION REPORT\n",
      "==============================\n",
      "\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "    Admiration       0.50      0.25      0.33        52\n",
      "     Annoyance       0.56      0.49      0.52        82\n",
      "  Appreciation       0.00      0.00      0.00         0\n",
      "      Approval       0.74      0.59      0.66        44\n",
      "     Confusion       1.00      0.56      0.71         9\n",
      "Disappointment       0.56      0.39      0.46        70\n",
      "   Disapproval       0.52      0.46      0.49        24\n",
      "       Disgust       0.00      0.00      0.00         0\n",
      "    Excitement       0.12      0.50      0.20         2\n",
      "          Fear       0.00      0.00      0.00         1\n",
      "   Frustration       0.33      0.70      0.45        37\n",
      "     Gratitude       0.58      0.64      0.61        11\n",
      "     Impressed       0.16      0.33      0.22        18\n",
      "   Indifferent       0.99      0.81      0.89       198\n",
      "           Joy       0.67      0.12      0.20        17\n",
      "   Realization       0.00      0.00      0.00         0\n",
      "    Relaxation       0.83      0.62      0.71         8\n",
      "        Relief       0.25      0.33      0.29         3\n",
      "  Satisfaction       0.39      0.77      0.52        26\n",
      "      Surprise       0.20      0.20      0.20         5\n",
      "\n",
      "      accuracy                           0.58       607\n",
      "     macro avg       0.42      0.39      0.37       607\n",
      "  weighted avg       0.68      0.58      0.61       607\n",
      "\n",
      "\n",
      "Saved â†’ emotion_eval_results_cleaned_300.csv\n"
     ]
    }
   ],
   "source": [
    "## For EMOTION-ONLY evaluation\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from collections import Counter\n",
    "\n",
    "# -----------------------------\n",
    "# Paths\n",
    "# -----------------------------\n",
    "\n",
    "#GOLD_PATH = os.path.join(data_root, \"daniel_50.jsonl\")\n",
    "#PRED_PATH = os.path.join(results_root, \"gemini-flash\", \"gemini_emotion_only_daniel_50.jsonl\")\n",
    "\n",
    "GOLD_PATH = os.path.join(data_root, \"02_iteration_cleaned_300.jsonl\")\n",
    "\n",
    "# ---- derive input identifier ----\n",
    "input_name = os.path.splitext(os.path.basename(GOLD_PATH))[0]\n",
    "\n",
    "#PRED_PATH = os.path.join(results_root, \"gemini-flash\", \"{input_name}__gemini_emotion_only.jsonl\")\n",
    "PRED_PATH = os.path.join(results_root, \"claude\", \"claude_output.jsonl\")\n",
    "# -----------------------------\n",
    "# Load JSONL\n",
    "# -----------------------------\n",
    "def load_jsonl(path):\n",
    "    return [json.loads(line) for line in open(path, \"r\", encoding=\"utf-8\")]\n",
    "\n",
    "gold = load_jsonl(GOLD_PATH)\n",
    "pred = load_jsonl(PRED_PATH)\n",
    "\n",
    "# Map gold/pred by input text\n",
    "gold_map = {row[\"input\"]: row[\"output\"] for row in gold}\n",
    "pred_map = {row[\"input\"]: row[\"output\"] for row in pred}\n",
    "\n",
    "print(f\"Loaded gold rows: {len(gold_map)}\")\n",
    "print(f\"Loaded pred rows: {len(pred_map)}\")\n",
    "\n",
    "# -----------------------------\n",
    "# EMOTION-ONLY ALIGNMENT\n",
    "# -----------------------------\n",
    "gold_emotions = []\n",
    "pred_emotions = []\n",
    "\n",
    "for text, gold_list in gold_map.items():\n",
    "\n",
    "    pred_list = pred_map.get(text, [])\n",
    "\n",
    "    # In emotion-only mode:\n",
    "    # - Number of triples must be identical (same aspects + polarities)\n",
    "    # - We ignore gold emotions and only evaluate the predicted ones\n",
    "\n",
    "    if len(pred_list) != len(gold_list):\n",
    "        raise ValueError(f\"Triple count mismatch for: {text[:50]}...\")\n",
    "\n",
    "    for gold_triple, pred_triple in zip(gold_list, pred_list):\n",
    "\n",
    "        # Gold label = gold triple's emotion\n",
    "        gold_emotions.append(gold_triple[\"emotion\"])\n",
    "\n",
    "        # Pred emotion\n",
    "        pred_emotions.append(pred_triple[\"emotion\"])\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Compute Metrics (Emotion Only)\n",
    "# -----------------------------\n",
    "emotion_macro = f1_score(gold_emotions, pred_emotions, average=\"macro\", zero_division=0)\n",
    "emotion_micro = f1_score(gold_emotions, pred_emotions, average=\"micro\", zero_division=0)\n",
    "\n",
    "# Full match accuracy = correct emotion only\n",
    "correct = sum(1 for g, p in zip(gold_emotions, pred_emotions) if g == p)\n",
    "full_match_acc = correct / len(gold_emotions)\n",
    "\n",
    "# -----------------------------\n",
    "# PRINT REPORT\n",
    "# -----------------------------\n",
    "print(\"\\n==============================\")\n",
    "print(\"EMOTION-ONLY EVALUATION REPORT\")\n",
    "print(\"==============================\\n\")\n",
    "\n",
    "print(\"Emotion Macro-F1 :\", round(emotion_macro, 4))\n",
    "print(\"Emotion Micro-F1 :\", round(emotion_micro, 4))\n",
    "print(\"Exact Emotion Accuracy:\", round(full_match_acc, 4))\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(\"DETAILED EMOTION REPORT\")\n",
    "print(\"==============================\\n\")\n",
    "print(classification_report(gold_emotions, pred_emotions, zero_division=0))\n",
    "\n",
    "# Save CSV\n",
    "df = pd.DataFrame({\n",
    "    \"Emotion Macro F1\": [emotion_macro],\n",
    "    \"Emotion Micro F1\": [emotion_micro],\n",
    "    \"Emotion Accuracy\": [full_match_acc]\n",
    "})\n",
    "df.to_csv(f\"emotion_eval_results_{GOLD_PATH.split('/')[-1].replace('.jsonl', '')}.csv\", index=False)\n",
    "\n",
    "print(f\"\\nSaved â†’ emotion_eval_results_{GOLD_PATH.split('/')[-1].replace('.jsonl', '')}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a45cc69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded gold rows: 300\n",
      "Loaded pred rows: 300\n",
      "\n",
      "==============================\n",
      "DUPLICATE ROW INDICES â†’ GOLD FILE\n",
      "==============================\n",
      "\n",
      "TEXT: 'We were able to reserve a spot at the chef tasting bar with Morimoto who actually called in sick that night, but we were still charged full price.'\n",
      "1-based row numbers: [1, 202]\n",
      "\n",
      "TEXT: 'The bar was so crowded there was no point in ordering a drink, and when we were finally seated we got no service at all for 20 minutes.'\n",
      "1-based row numbers: [152, 302]\n",
      "\n",
      "\n",
      "==============================\n",
      "DUPLICATE ROW INDICES â†’ PRED FILE\n",
      "==============================\n",
      "\n",
      "TEXT: 'We were able to reserve a spot at the chef tasting bar with Morimoto who actually called in sick that night, but we were still charged full price.'\n",
      "1-based row numbers: [1, 202]\n",
      "\n",
      "TEXT: 'The bar was so crowded there was no point in ordering a drink, and when we were finally seated we got no service at all for 20 minutes.'\n",
      "1-based row numbers: [152, 302]\n",
      "\n",
      "\n",
      "==============================\n",
      "DUPLICATES IN BOTH FILES (WITH ROW NUMBERS)\n",
      "==============================\n",
      "\n",
      "TEXT: 'The bar was so crowded there was no point in ordering a drink, and when we were finally seated we got no service at all for 20 minutes.'\n",
      " â†’ GOLD rows: [152, 302]\n",
      " â†’ PRED rows: [152, 302]\n",
      "\n",
      "-----------------------------------\n",
      "\n",
      "TEXT: 'We were able to reserve a spot at the chef tasting bar with Morimoto who actually called in sick that night, but we were still charged full price.'\n",
      " â†’ GOLD rows: [1, 202]\n",
      " â†’ PRED rows: [1, 202]\n",
      "\n",
      "-----------------------------------\n",
      "\n",
      "==============================\n",
      "FINAL EVALUATION REPORT\n",
      "==============================\n",
      "\n",
      "ASPECT CLASSIFICATION\n",
      "------------------------------\n",
      "Macro F1: 0.4346\n",
      "\n",
      "POLARITY CLASSIFICATION\n",
      "------------------------------\n",
      "Macro F1: 0.4324\n",
      "\n",
      "EMOTION CLASSIFICATION\n",
      "------------------------------\n",
      "Emotion Macro-F1 : 0.2475\n",
      "Emotion Micro-F1 : 0.2965\n",
      "\n",
      "FULL ABSA TRIPLE MATCH\n",
      "------------------------------\n",
      "Full ABSA Accuracy: 0.2175\n",
      "\n",
      "==============================\n",
      "DETAILED CLASSIFICATION REPORT\n",
      "==============================\n",
      "\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "    Admiration       0.49      0.37      0.42        52\n",
      "     Annoyance       0.35      0.55      0.43        82\n",
      "  Appreciation       0.00      0.00      0.00         0\n",
      "      Approval       0.35      0.41      0.38        44\n",
      "     Confusion       0.33      0.22      0.27         9\n",
      "Disappointment       0.30      0.39      0.34        70\n",
      "   Disapproval       0.36      0.17      0.23        24\n",
      "       Disgust       0.00      0.00      0.00         0\n",
      "    Excitement       0.00      0.00      0.00         2\n",
      "          Fear       1.00      1.00      1.00         1\n",
      "   Frustration       0.29      0.32      0.30        37\n",
      "     Gratitude       0.43      0.55      0.48        11\n",
      "     Impressed       0.24      0.22      0.23        18\n",
      "   Indifferent       0.69      0.10      0.18       198\n",
      "           Joy       0.14      0.06      0.08        17\n",
      "   Realization       0.00      0.00      0.00         0\n",
      "    Relaxation       0.50      0.50      0.50         8\n",
      "        Relief       0.00      0.00      0.00         3\n",
      "  Satisfaction       0.26      0.65      0.37        26\n",
      "      Surprise       0.00      0.00      0.00         5\n",
      "          none       0.00      0.00      0.00         0\n",
      "\n",
      "      accuracy                           0.30       607\n",
      "     macro avg       0.27      0.26      0.25       607\n",
      "  weighted avg       0.45      0.30      0.29       607\n",
      "\n",
      "\n",
      "Saved â†’ eval_results_cleaned_300.csv\n"
     ]
    }
   ],
   "source": [
    "## Aspect-Polarity-Emotion evaluation\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from collections import Counter\n",
    "\n",
    "# -----------------------------\n",
    "# Paths\n",
    "# -----------------------------\n",
    "\n",
    "#GOLD_PATH = os.path.join(data_root, \"daniel_50.jsonl\")\n",
    "#PRED_PATH = os.path.join(results_root, \"gemini-flash\", \"gemini_annotated_aspect_polarity_emotions_daniel_50.jsonl\")\n",
    "\n",
    "GOLD_PATH = os.path.join(data_root, \"cleaned_300.jsonl\")\n",
    "PRED_PATH = os.path.join(results_root, \"gemini-flash\", \"gemini_annotated_aspect_polarity_emotions_300.jsonl\")\n",
    "# -----------------------------\n",
    "# Load JSONL\n",
    "# -----------------------------\n",
    "def load_jsonl(path):\n",
    "    return [json.loads(line) for line in open(path, \"r\", encoding=\"utf-8\")]\n",
    "\n",
    "gold = load_jsonl(GOLD_PATH)\n",
    "pred = load_jsonl(PRED_PATH)\n",
    "\n",
    "# Map gold/pred by input text\n",
    "gold_map = {row[\"input\"]: row[\"output\"] for row in gold}\n",
    "pred_map = {row[\"input\"]: row[\"output\"] for row in pred}\n",
    "\n",
    "print(f\"Loaded gold rows: {len(gold_map)}\")\n",
    "print(f\"Loaded pred rows: {len(pred_map)}\")\n",
    "\n",
    "\n",
    "# -----------------------------------\n",
    "# Duplicate detection helper\n",
    "# -----------------------------------\n",
    "def find_duplicates(rows, name):\n",
    "    counter = Counter([row[\"input\"] for row in rows])\n",
    "    dups = [inp for inp, c in counter.items() if c > 1]\n",
    "\n",
    "    print(f\"\\n==============================\")\n",
    "    print(f\"DUPLICATE CHECK â†’ {name}\")\n",
    "    print(\"==============================\")\n",
    "\n",
    "    if not dups:\n",
    "        print(\"No duplicates found.\\n\")\n",
    "        return {}\n",
    "\n",
    "    print(f\"Found {len(dups)} duplicate input texts:\")\n",
    "    for d in dups:\n",
    "        print(f\" - {d!r}  (x{counter[d]})\")\n",
    "    print()\n",
    "\n",
    "    # return mapping of input â†’ list of rows\n",
    "    dup_map = {d: [r for r in rows if r[\"input\"] == d] for d in dups}\n",
    "    return dup_map\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def find_duplicate_indices_1_based(rows, name):\n",
    "    index_map = defaultdict(list)\n",
    "\n",
    "    for i, row in enumerate(rows):\n",
    "        index_map[row[\"input\"]].append(i + 1)   # convert to 1-based\n",
    "\n",
    "    duplicates = {text: idxs for text, idxs in index_map.items() if len(idxs) > 1}\n",
    "\n",
    "    print(f\"\\n==============================\")\n",
    "    print(f\"DUPLICATE ROW INDICES â†’ {name}\")\n",
    "    print(\"==============================\")\n",
    "\n",
    "    if not duplicates:\n",
    "        print(\"No duplicates found.\\n\")\n",
    "        return duplicates\n",
    "\n",
    "    for text, idxs in duplicates.items():\n",
    "        print(f\"\\nTEXT: {text!r}\")\n",
    "        print(f\"1-based row numbers: {idxs}\")\n",
    "\n",
    "    print()\n",
    "    return duplicates\n",
    "\n",
    "\n",
    "gold_dup_indices = find_duplicate_indices_1_based(gold, \"GOLD FILE\")\n",
    "pred_dup_indices = find_duplicate_indices_1_based(pred, \"PRED FILE\")\n",
    "\n",
    "# -----------------------------------\n",
    "# Duplicates appearing in BOTH (with indices)\n",
    "# -----------------------------------\n",
    "print(\"\\n==============================\")\n",
    "print(\"DUPLICATES IN BOTH FILES (WITH ROW NUMBERS)\")\n",
    "print(\"==============================\")\n",
    "\n",
    "common = set(gold_dup_indices.keys()) & set(pred_dup_indices.keys())\n",
    "\n",
    "if not common:\n",
    "    print(\"No overlapping duplicates.\\n\")\n",
    "else:\n",
    "    for text in common:\n",
    "        print(f\"\\nTEXT: {text!r}\")\n",
    "        print(f\" â†’ GOLD rows: {gold_dup_indices[text]}\")\n",
    "        print(f\" â†’ PRED rows: {pred_dup_indices[text]}\")\n",
    "        print(\"\\n-----------------------------------\")\n",
    "\n",
    "# -----------------------------\n",
    "# Triple alignment\n",
    "# -----------------------------\n",
    "def align_triples(gold_list, pred_list):\n",
    "    \"\"\"\n",
    "    Ensures aligned triples for scoring:\n",
    "    - If pred has MORE triples â†’ truncate\n",
    "    - If pred has FEWER triples â†’ pad with ('none','none','none')\n",
    "    \"\"\"\n",
    "    g = [(t[\"aspect\"], t[\"polarity\"], t[\"emotion\"]) for t in gold_list]\n",
    "    p = [(t[\"aspect\"], t[\"polarity\"], t[\"emotion\"]) for t in pred_list]\n",
    "\n",
    "    gold_n = len(g)\n",
    "    pred_n = len(p)\n",
    "\n",
    "    # truncate hallucinations\n",
    "    if pred_n > gold_n:\n",
    "        p = p[:gold_n]\n",
    "\n",
    "    # pad missing predictions\n",
    "    if pred_n < gold_n:\n",
    "        pad = [(\"none\", \"none\", \"none\")] * (gold_n - pred_n)\n",
    "        p.extend(pad)\n",
    "\n",
    "    return g, p\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Collect aligned labels\n",
    "# -----------------------------\n",
    "all_gold_as, all_pred_as = [], []\n",
    "all_gold_pol, all_pred_pol = [], []\n",
    "all_gold_emo, all_pred_emo = [], []\n",
    "\n",
    "gold_triples_full = []\n",
    "pred_triples_full = []\n",
    "\n",
    "for text, gold_list in gold_map.items():\n",
    "\n",
    "    pred_list = pred_map.get(text, [])\n",
    "\n",
    "    g_aligned, p_aligned = align_triples(gold_list, pred_list)\n",
    "\n",
    "    for (ga, gp, ge), (pa, pp, pe) in zip(g_aligned, p_aligned):\n",
    "        all_gold_as.append(ga)\n",
    "        all_gold_pol.append(gp)\n",
    "        all_gold_emo.append(ge)\n",
    "\n",
    "        all_pred_as.append(pa)\n",
    "        all_pred_pol.append(pp)\n",
    "        all_pred_emo.append(pe)\n",
    "\n",
    "        gold_triples_full.append((ga, gp, ge))\n",
    "        pred_triples_full.append((pa, pp, pe))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Compute metrics\n",
    "# -----------------------------\n",
    "aspect_f1 = f1_score(all_gold_as, all_pred_as, average=\"macro\", zero_division=0)\n",
    "polarity_f1 = f1_score(all_gold_pol, all_pred_pol, average=\"macro\", zero_division=0)\n",
    "emotion_macro = f1_score(all_gold_emo, all_pred_emo, average=\"macro\", zero_division=0)\n",
    "emotion_micro = f1_score(all_gold_emo, all_pred_emo, average=\"micro\", zero_division=0)\n",
    "\n",
    "# Full ABSA accuracy (exact triple match)\n",
    "match = sum(1 for g, p in zip(gold_triples_full, pred_triples_full) if g == p)\n",
    "exact_acc = match / len(gold_triples_full)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Pretty PRINT\n",
    "# -----------------------------\n",
    "print(\"\\n==============================\")\n",
    "print(\"FINAL EVALUATION REPORT\")\n",
    "print(\"==============================\\n\")\n",
    "\n",
    "print(\"ASPECT CLASSIFICATION\")\n",
    "print(\"------------------------------\")\n",
    "print(f\"Macro F1: {aspect_f1:.4f}\\n\")\n",
    "\n",
    "print(\"POLARITY CLASSIFICATION\")\n",
    "print(\"------------------------------\")\n",
    "print(f\"Macro F1: {polarity_f1:.4f}\\n\")\n",
    "\n",
    "print(\"EMOTION CLASSIFICATION\")\n",
    "print(\"------------------------------\")\n",
    "print(f\"Emotion Macro-F1 : {emotion_macro:.4f}\")\n",
    "print(f\"Emotion Micro-F1 : {emotion_micro:.4f}\\n\")\n",
    "\n",
    "print(\"FULL ABSA TRIPLE MATCH\")\n",
    "print(\"------------------------------\")\n",
    "print(f\"Full ABSA Accuracy: {exact_acc:.4f}\\n\")\n",
    "\n",
    "print(\"==============================\")\n",
    "print(\"DETAILED CLASSIFICATION REPORT\")\n",
    "print(\"==============================\\n\")\n",
    "print(classification_report(all_gold_emo, all_pred_emo, zero_division=0))\n",
    "\n",
    "\n",
    "# Save optional CSV\n",
    "df = pd.DataFrame({\n",
    "    \"Aspect F1\":      [aspect_f1],\n",
    "    \"Polarity F1\":    [polarity_f1],\n",
    "    \"Emotion F1\":     [emotion_macro],\n",
    "    \"Emotion Micro\":  [emotion_micro],\n",
    "    \"Full ABSA Acc\":  [exact_acc]\n",
    "})\n",
    "df.to_csv(f\"eval_results_absa_{GOLD_PATH.split('/')[-1].replace('.jsonl', '')}.csv\", index=False)\n",
    "\n",
    "print(f\"\\nSaved â†’ eval_results_{GOLD_PATH.split('/')[-1].replace('.jsonl', '')}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
