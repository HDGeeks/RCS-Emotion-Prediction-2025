{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027a8ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We fine-tune a pretrained transformer encoder to perform aspect-based emotion classification.â€\n",
    "\"\"\"\n",
    "Modeling Rationale\n",
    "\n",
    "Although classical machine learning models such as Logistic Regression or\n",
    "Support Vector Machines can be trained from scratch for text classification,\n",
    "they rely on shallow lexical representations (e.g., bag-of-words or TFâ€“IDF)\n",
    "and therefore lack an explicit notion of context, word order, and semantic\n",
    "composition.\n",
    "\n",
    "Emotion classification, and especially aspect-based emotion classification,\n",
    "is inherently context-dependent. The same sentence may express different\n",
    "emotions depending on the referenced aspect, and emotional meaning is often\n",
    "conveyed implicitly through contrast, expectation, or discourse structure\n",
    "rather than through isolated keywords.\n",
    "\n",
    "Training a model from scratch under these conditions would require learning\n",
    "both linguistic structure and emotional semantics simultaneously, which is\n",
    "impractical with limited labeled data. Instead, we fine-tune a pretrained\n",
    "transformer encoder that already captures rich syntactic and semantic\n",
    "regularities of natural language. Fine-tuning allows the model to adapt this\n",
    "general linguistic knowledge to the specific task of emotion classification\n",
    "with substantially fewer examples and improved generalization.\n",
    "\n",
    "For these reasons, a fine-tuned transformer-based classifier is more suitable\n",
    "than classical models trained from scratch for the task considered here.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8458e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model Choice Justification\n",
    "\n",
    "We use DistilRoBERTa as the underlying language encoder for emotion classification.\n",
    "DistilRoBERTa is a distilled version of RoBERTa that retains most of its\n",
    "representational power while being significantly smaller and faster. This makes\n",
    "it well-suited for experimental settings with limited computational resources\n",
    "and moderate-sized datasets, while still providing strong contextual language\n",
    "understanding.\n",
    "\n",
    "RoBERTa-based models are particularly effective for sentiment and emotion-related\n",
    "tasks because they are pretrained on large amounts of diverse text using a\n",
    "masked language modeling objective that captures nuanced lexical, syntactic,\n",
    "and semantic relationships. These properties are critical for emotion detection,\n",
    "where affect is often expressed implicitly rather than through explicit emotion\n",
    "words.\n",
    "\n",
    "DistilRoBERTa offers a practical trade-off between performance and efficiency:\n",
    "it enables stable fine-tuning, faster iteration, and reduced overfitting risk\n",
    "compared to larger models, without sacrificing the core benefits of contextual\n",
    "representations.\n",
    "\n",
    "Alternative pretrained models could also be used within the same framework.\n",
    "For example:\n",
    "- BERT-base can serve as a widely adopted baseline, though it is generally\n",
    "  weaker than RoBERTa-style models on sentiment-oriented tasks.\n",
    "- RoBERTa-base may yield slightly higher performance at the cost of increased\n",
    "  memory usage and slower training.\n",
    "- ALBERT provides parameter sharing for efficiency but can be less stable during\n",
    "  fine-tuning.\n",
    "- Domain-specific models (e.g., sentiment-adapted or review-trained transformers)\n",
    "  may further improve performance if available.\n",
    "\n",
    "The choice of DistilRoBERTa therefore reflects a deliberate balance between model\n",
    "capacity, training stability, computational efficiency, and suitability for\n",
    "aspect-based emotion classification.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db02edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "PROJECT_MARKERS = (\"src\", \"data\", \"prompts\", \"results\")\n",
    "\n",
    "def find_project_root(start_path):\n",
    "    current = os.path.abspath(start_path)\n",
    "\n",
    "    while True:\n",
    "        if all(os.path.isdir(os.path.join(current, m)) for m in PROJECT_MARKERS):\n",
    "            return current\n",
    "\n",
    "        parent = os.path.dirname(current)\n",
    "        if parent == current:\n",
    "            raise RuntimeError(\"Project root not found\")\n",
    "\n",
    "        current = parent\n",
    "\n",
    "\n",
    "# ---- execution directory (cwd) ----\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# ---- safe starting point ----\n",
    "try:\n",
    "    start_path = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    start_path = cwd\n",
    "\n",
    "\n",
    "# ---- resolve canonical paths ----\n",
    "project_root = find_project_root(start_path)\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "src_root     = os.path.join(project_root, \"src\", \"daniel\", \"gemini\")\n",
    "data_root    = os.path.join(project_root, \"data\", \"MAMS-ACSA\", \"raw\", \"data_jsonl\", \"annotated\")\n",
    "schemas_root = os.path.join(project_root, \"data\", \"MAMS-ACSA\", \"raw\", \"data_jsonl\", \"schema\")\n",
    "prompts_root = os.path.join(project_root, \"prompts\", \"daniel\", \"llama\")\n",
    "utils_root   = os.path.join(project_root, \"utils\")\n",
    "results_root = os.path.join(project_root, \"results\", \"daniel\")\n",
    "\n",
    "print(\n",
    "    f\"ðŸ“‚ cwd          : {cwd}\\n\"\n",
    "    f\"ðŸ“‚ Project root : {project_root}\\n\"\n",
    "    f\"ðŸ“‚ Source root  : {src_root}\\n\"\n",
    "    f\"ðŸ“‚ Data root    : {data_root}\\n\"\n",
    "    f\"ðŸ“‚ Prompts root : {prompts_root}\\n\"\n",
    "    f\"ðŸ“‚ Utils root   : {utils_root}\\n\"\n",
    "    f\"ðŸ“‚ Results root : {results_root}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3d2e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d90730",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = Path(data_root) / \"train.jsonl\"   # adjust filename \n",
    "\n",
    "rows = []\n",
    "with open(data_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        rows.append(json.loads(line))\n",
    "\n",
    "len(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1720e806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def light_check(rows, name):\n",
    "    for i, r in enumerate(rows):\n",
    "        assert isinstance(r, dict), f\"{name}[{i}] is not a dict\"\n",
    "\n",
    "        # input\n",
    "        assert \"input\" in r, f\"{name}[{i}] missing 'input'\"\n",
    "        assert isinstance(r[\"input\"], str), f\"{name}[{i}]['input'] not a string\"\n",
    "        assert r[\"input\"].strip(), f\"{name}[{i}] empty 'input'\"\n",
    "\n",
    "        # output\n",
    "        assert \"output\" in r, f\"{name}[{i}] missing 'output'\"\n",
    "        assert isinstance(r[\"output\"], list), f\"{name}[{i}]['output'] not a list\"\n",
    "        assert len(r[\"output\"]) > 0, f\"{name}[{i}] empty 'output' list\"\n",
    "\n",
    "        # each label item\n",
    "        for j, o in enumerate(r[\"output\"]):\n",
    "            assert isinstance(o, dict), f\"{name}[{i}]['output'][{j}] not a dict\"\n",
    "            for k in (\"aspect\", \"polarity\", \"emotion\"):\n",
    "                assert k in o, f\"{name}[{i}]['output'][{j}] missing '{k}'\"\n",
    "                assert isinstance(o[k], str), f\"{name}[{i}]['output'][{j}]['{k}'] not a string\"\n",
    "                assert o[k].strip(), f\"{name}[{i}]['output'][{j}] empty '{k}'\"\n",
    "\n",
    "    print(f\"{name}: {len(rows)} rows passed\")\n",
    "\n",
    "\n",
    "light_check(rows=rows, name=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400a8efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand rows because each input can have multiple aspect-emotion-polarity labels\n",
    "records = []\n",
    "\n",
    "for r in rows:\n",
    "    text = r[\"input\"]\n",
    "    for o in r[\"output\"]:\n",
    "        records.append({\n",
    "            \"text\": text,\n",
    "            \"aspect\": o[\"aspect\"],\n",
    "            \"emotion\": o[\"emotion\"],\n",
    "            \"polarity\": o[\"polarity\"]\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b0e3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"emotion\"] = df[\"emotion\"].replace({\"mentioned_only\": \"neutral\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5db2b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"emotion\"].value_counts() # This shows the distribution of emotion labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea949a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_encoder = LabelEncoder()\n",
    "polarity_encoder = LabelEncoder()\n",
    "\n",
    "df[\"emotion_id\"]  = emotion_encoder.fit_transform(df[\"emotion\"])\n",
    "df[\"polarity_id\"] = polarity_encoder.fit_transform(df[\"polarity\"])\n",
    "\n",
    "num_emotions  = len(emotion_encoder.classes_)\n",
    "num_polarity  = len(polarity_encoder.classes_)\n",
    "\n",
    "emotion_encoder.classes_, polarity_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79b722d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_texts = df[\"text\"].unique()\n",
    "\n",
    "train_texts, val_texts = train_test_split(\n",
    "    unique_texts,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_df = df[df[\"text\"].isin(train_texts)]\n",
    "val_df   = df[df[\"text\"].isin(val_texts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca152a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"distilroberta-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f1ca7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for aspect-conditioned emotion and polarity classification.\n",
    "\n",
    "    This dataset converts a tabular representation of reviews into model-ready\n",
    "    tensors. Each item corresponds to a single (text, aspect) pair and produces:\n",
    "\n",
    "    - tokenized input text, where the aspect is explicitly prepended to guide\n",
    "      the model's attention,\n",
    "    - attention masks for the transformer encoder,\n",
    "    - numeric labels for emotion (primary task),\n",
    "    - numeric labels for polarity (auxiliary task).\n",
    "\n",
    "    The dataset is used by the HuggingFace Trainer to dynamically construct\n",
    "    training and evaluation batches during fine-tuning.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, tokenizer, max_len=256):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.loc[idx]\n",
    "\n",
    "        text = f\"ASPECT: {row['aspect']} | TEXT: {row['text']}\"\n",
    "\n",
    "        enc = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
    "            \"emotion_labels\": torch.tensor(row[\"emotion_id\"]),\n",
    "            \"polarity_labels\": torch.tensor(row[\"polarity_id\"])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00506da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionPolarityModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-task transformer model for aspect-conditioned emotion classification.\n",
    "\n",
    "    The model consists of a shared pretrained language encoder and two\n",
    "    task-specific classification heads:\n",
    "    - an emotion head (primary task),\n",
    "    - a polarity head (auxiliary task).\n",
    "\n",
    "    Both tasks are trained jointly using a weighted loss, where polarity\n",
    "    supervision acts as a regularizer to improve the quality of learned\n",
    "    representations. During evaluation, only emotion predictions are used\n",
    "    for performance reporting.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name, num_emotions, num_polarity):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        hidden = self.encoder.config.hidden_size\n",
    "\n",
    "        self.emotion_head  = torch.nn.Linear(hidden, num_emotions)\n",
    "        self.polarity_head = torch.nn.Linear(hidden, num_polarity)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, emotion_labels=None, polarity_labels=None):\n",
    "        out = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        cls = out.last_hidden_state[:, 0]\n",
    "\n",
    "        emotion_logits  = self.emotion_head(cls)\n",
    "        polarity_logits = self.polarity_head(cls)\n",
    "\n",
    "        loss = None\n",
    "        if emotion_labels is not None:\n",
    "            loss_e = torch.nn.functional.cross_entropy(emotion_logits, emotion_labels)\n",
    "            loss_p = torch.nn.functional.cross_entropy(polarity_logits, polarity_labels)\n",
    "            loss = loss_e + 0.3 * loss_p   # polarity = auxiliary\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"emotion_logits\": emotion_logits,\n",
    "            \"polarity_logits\": polarity_logits\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4840438d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = EmotionDataset(train_df, tokenizer)\n",
    "val_ds   = EmotionDataset(val_df, tokenizer)\n",
    "\n",
    "model = EmotionPolarityModel(\n",
    "    MODEL_NAME,\n",
    "    num_emotions=num_emotions,\n",
    "    num_polarity=num_polarity\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd281c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    return {\n",
    "        \"macro_f1\": f1_score(labels, preds, average=\"macro\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1205e16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        outputs = model(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            emotion_labels=inputs[\"emotion_labels\"],\n",
    "            polarity_labels=inputs[\"polarity_labels\"]\n",
    "        )\n",
    "        return (outputs[\"loss\"], outputs) if return_outputs else outputs[\"loss\"]\n",
    "\n",
    "    def prediction_step(self, model, inputs, prediction_loss_only=False, ignore_keys=None):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"]\n",
    "            )\n",
    "\n",
    "        logits = outputs[\"emotion_logits\"]\n",
    "        labels = inputs[\"emotion_labels\"]\n",
    "\n",
    "        return None, logits.cpu().numpy(), labels.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47000df",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=results_root,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = EmotionTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
