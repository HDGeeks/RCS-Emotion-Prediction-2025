{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db02edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "PROJECT_MARKERS = (\"src\", \"data\", \"prompts\", \"results\")\n",
    "\n",
    "def find_project_root(start_path):\n",
    "    current = os.path.abspath(start_path)\n",
    "\n",
    "    while True:\n",
    "        if all(os.path.isdir(os.path.join(current, m)) for m in PROJECT_MARKERS):\n",
    "            return current\n",
    "\n",
    "        parent = os.path.dirname(current)\n",
    "        if parent == current:\n",
    "            raise RuntimeError(\"Project root not found\")\n",
    "\n",
    "        current = parent\n",
    "\n",
    "\n",
    "# ---- execution directory (cwd) ----\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# ---- safe starting point ----\n",
    "try:\n",
    "    start_path = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    start_path = cwd\n",
    "\n",
    "\n",
    "# ---- resolve canonical paths ----\n",
    "project_root = find_project_root(start_path)\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "src_root     = os.path.join(project_root, \"src\", \"daniel\", \"gemini\")\n",
    "data_root    = os.path.join(project_root, \"data\", \"MAMS-ACSA\", \"raw\", \"data_jsonl\", \"annotated\")\n",
    "schemas_root = os.path.join(project_root, \"data\", \"MAMS-ACSA\", \"raw\", \"data_jsonl\", \"schema\")\n",
    "prompts_root = os.path.join(project_root, \"prompts\", \"daniel\", \"llama\")\n",
    "utils_root   = os.path.join(project_root, \"utils\")\n",
    "results_root = os.path.join(project_root, \"results\", \"daniel\")\n",
    "\n",
    "print(\n",
    "    f\"ðŸ“‚ cwd          : {cwd}\\n\"\n",
    "    f\"ðŸ“‚ Project root : {project_root}\\n\"\n",
    "    f\"ðŸ“‚ Source root  : {src_root}\\n\"\n",
    "    f\"ðŸ“‚ Data root    : {data_root}\\n\"\n",
    "    f\"ðŸ“‚ Prompts root : {prompts_root}\\n\"\n",
    "    f\"ðŸ“‚ Utils root   : {utils_root}\\n\"\n",
    "    f\"ðŸ“‚ Results root : {results_root}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3d2e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d90730",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = Path(data_root) / \"train.jsonl\"   # adjust filename if needed\n",
    "\n",
    "rows = []\n",
    "with open(data_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        rows.append(json.loads(line))\n",
    "\n",
    "len(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1720e806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def light_check(rows, name):\n",
    "    for i, r in enumerate(rows):\n",
    "        assert isinstance(r, dict), f\"{name}[{i}] is not a dict\"\n",
    "\n",
    "        # input\n",
    "        assert \"input\" in r, f\"{name}[{i}] missing 'input'\"\n",
    "        assert isinstance(r[\"input\"], str), f\"{name}[{i}]['input'] not a string\"\n",
    "        assert r[\"input\"].strip(), f\"{name}[{i}] empty 'input'\"\n",
    "\n",
    "        # output\n",
    "        assert \"output\" in r, f\"{name}[{i}] missing 'output'\"\n",
    "        assert isinstance(r[\"output\"], list), f\"{name}[{i}]['output'] not a list\"\n",
    "        assert len(r[\"output\"]) > 0, f\"{name}[{i}] empty 'output' list\"\n",
    "\n",
    "        # each label item\n",
    "        for j, o in enumerate(r[\"output\"]):\n",
    "            assert isinstance(o, dict), f\"{name}[{i}]['output'][{j}] not a dict\"\n",
    "            for k in (\"aspect\", \"polarity\", \"emotion\"):\n",
    "                assert k in o, f\"{name}[{i}]['output'][{j}] missing '{k}'\"\n",
    "                assert isinstance(o[k], str), f\"{name}[{i}]['output'][{j}]['{k}'] not a string\"\n",
    "                assert o[k].strip(), f\"{name}[{i}]['output'][{j}] empty '{k}'\"\n",
    "\n",
    "    print(f\"{name}: {len(rows)} rows passed\")\n",
    "\n",
    "\n",
    "light_check(rows=rows, name=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400a8efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "\n",
    "for r in rows:\n",
    "    text = r[\"input\"]\n",
    "    for o in r[\"output\"]:\n",
    "        records.append({\n",
    "            \"text\": text,\n",
    "            \"aspect\": o[\"aspect\"],\n",
    "            \"emotion\": o[\"emotion\"],\n",
    "            \"polarity\": o[\"polarity\"]\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b0e3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"emotion\"] = df[\"emotion\"].replace({\"mentioned_only\": \"neutral\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea949a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_encoder = LabelEncoder()\n",
    "polarity_encoder = LabelEncoder()\n",
    "\n",
    "df[\"emotion_id\"]  = emotion_encoder.fit_transform(df[\"emotion\"])\n",
    "df[\"polarity_id\"] = polarity_encoder.fit_transform(df[\"polarity\"])\n",
    "\n",
    "num_emotions  = len(emotion_encoder.classes_)\n",
    "num_polarity  = len(polarity_encoder.classes_)\n",
    "\n",
    "emotion_encoder.classes_, polarity_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79b722d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_texts = df[\"text\"].unique()\n",
    "\n",
    "train_texts, val_texts = train_test_split(\n",
    "    unique_texts,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_df = df[df[\"text\"].isin(train_texts)]\n",
    "val_df   = df[df[\"text\"].isin(val_texts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca152a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"distilroberta-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f1ca7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=256):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.loc[idx]\n",
    "\n",
    "        text = f\"ASPECT: {row['aspect']} | TEXT: {row['text']}\"\n",
    "\n",
    "        enc = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
    "            \"emotion_labels\": torch.tensor(row[\"emotion_id\"]),\n",
    "            \"polarity_labels\": torch.tensor(row[\"polarity_id\"])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00506da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionPolarityModel(torch.nn.Module):\n",
    "    def __init__(self, model_name, num_emotions, num_polarity):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        hidden = self.encoder.config.hidden_size\n",
    "\n",
    "        self.emotion_head  = torch.nn.Linear(hidden, num_emotions)\n",
    "        self.polarity_head = torch.nn.Linear(hidden, num_polarity)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, emotion_labels=None, polarity_labels=None):\n",
    "        out = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        cls = out.last_hidden_state[:, 0]\n",
    "\n",
    "        emotion_logits  = self.emotion_head(cls)\n",
    "        polarity_logits = self.polarity_head(cls)\n",
    "\n",
    "        loss = None\n",
    "        if emotion_labels is not None:\n",
    "            loss_e = torch.nn.functional.cross_entropy(emotion_logits, emotion_labels)\n",
    "            loss_p = torch.nn.functional.cross_entropy(polarity_logits, polarity_labels)\n",
    "            loss = loss_e + 0.3 * loss_p   # polarity = auxiliary\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"emotion_logits\": emotion_logits,\n",
    "            \"polarity_logits\": polarity_logits\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4840438d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = EmotionDataset(train_df, tokenizer)\n",
    "val_ds   = EmotionDataset(val_df, tokenizer)\n",
    "\n",
    "model = EmotionPolarityModel(\n",
    "    MODEL_NAME,\n",
    "    num_emotions=num_emotions,\n",
    "    num_polarity=num_polarity\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd281c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    return {\n",
    "        \"macro_f1\": f1_score(labels, preds, average=\"macro\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1205e16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        outputs = model(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            emotion_labels=inputs[\"emotion_labels\"],\n",
    "            polarity_labels=inputs[\"polarity_labels\"]\n",
    "        )\n",
    "        return (outputs[\"loss\"], outputs) if return_outputs else outputs[\"loss\"]\n",
    "\n",
    "    def prediction_step(self, model, inputs, prediction_loss_only=False, ignore_keys=None):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"]\n",
    "            )\n",
    "\n",
    "        logits = outputs[\"emotion_logits\"]\n",
    "        labels = inputs[\"emotion_labels\"]\n",
    "\n",
    "        return None, logits.cpu().numpy(), labels.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47000df",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=results_root,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = EmotionTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
