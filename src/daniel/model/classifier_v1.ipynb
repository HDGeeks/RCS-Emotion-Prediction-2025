{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027a8ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We fine-tune a pretrained transformer encoder to perform aspect-based emotion classification.”\n",
    "\"\"\"\n",
    "Modeling Rationale\n",
    "\n",
    "Although classical machine learning models such as Logistic Regression or\n",
    "Support Vector Machines can be trained from scratch for text classification,\n",
    "they rely on shallow lexical representations (e.g., bag-of-words or TF–IDF)\n",
    "and therefore lack an explicit notion of context, word order, and semantic\n",
    "composition.\n",
    "\n",
    "Emotion classification, and especially aspect-based emotion classification,\n",
    "is inherently context-dependent. The same sentence may express different\n",
    "emotions depending on the referenced aspect, and emotional meaning is often\n",
    "conveyed implicitly through contrast, expectation, or discourse structure\n",
    "rather than through isolated keywords.\n",
    "\n",
    "Training a model from scratch under these conditions would require learning\n",
    "both linguistic structure and emotional semantics simultaneously, which is\n",
    "impractical with limited labeled data. Instead, we fine-tune a pretrained\n",
    "transformer encoder that already captures rich syntactic and semantic\n",
    "regularities of natural language. Fine-tuning allows the model to adapt this\n",
    "general linguistic knowledge to the specific task of emotion classification\n",
    "with substantially fewer examples and improved generalization.\n",
    "\n",
    "For these reasons, a fine-tuned transformer-based classifier is more suitable\n",
    "than classical models trained from scratch for the task considered here.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8458e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model Choice Justification\n",
    "\n",
    "We use DistilRoBERTa as the underlying language encoder for emotion classification.\n",
    "DistilRoBERTa is a distilled version of RoBERTa that retains most of its\n",
    "representational power while being significantly smaller and faster. This makes\n",
    "it well-suited for experimental settings with limited computational resources\n",
    "and moderate-sized datasets, while still providing strong contextual language\n",
    "understanding.\n",
    "\n",
    "RoBERTa-based models are particularly effective for sentiment and emotion-related\n",
    "tasks because they are pretrained on large amounts of diverse text using a\n",
    "masked language modeling objective that captures nuanced lexical, syntactic,\n",
    "and semantic relationships. These properties are critical for emotion detection,\n",
    "where affect is often expressed implicitly rather than through explicit emotion\n",
    "words.\n",
    "\n",
    "DistilRoBERTa offers a practical trade-off between performance and efficiency:\n",
    "it enables stable fine-tuning, faster iteration, and reduced overfitting risk\n",
    "compared to larger models, without sacrificing the core benefits of contextual\n",
    "representations.\n",
    "\n",
    "Alternative pretrained models could also be used within the same framework.\n",
    "For example:\n",
    "- BERT-base can serve as a widely adopted baseline, though it is generally\n",
    "  weaker than RoBERTa-style models on sentiment-oriented tasks.\n",
    "- RoBERTa-base may yield slightly higher performance at the cost of increased\n",
    "  memory usage and slower training.\n",
    "- ALBERT provides parameter sharing for efficiency but can be less stable during\n",
    "  fine-tuning.\n",
    "- Domain-specific models (e.g., sentiment-adapted or review-trained transformers)\n",
    "  may further improve performance if available.\n",
    "\n",
    "The choice of DistilRoBERTa therefore reflects a deliberate balance between model\n",
    "capacity, training stability, computational efficiency, and suitability for\n",
    "aspect-based emotion classification.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38c24fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Aspect-Conditioned Emotion Classification\n",
    "Supervised Fine-Tuning of a Pretrained Transformer (Multi-Task Learning)\n",
    "\n",
    "This notebook implements supervised fine-tuning of a pretrained\n",
    "transformer encoder for aspect-conditioned emotion classification\n",
    "on restaurant reviews.\n",
    "\n",
    "The task is formulated as a classification problem, where each input\n",
    "consists of a review text and a target aspect, and the model predicts\n",
    "the associated emotion label.\n",
    "\n",
    "Polarity classification is included as an auxiliary task during training\n",
    "to regularize the shared representation via multi-task learning.\n",
    "The auxiliary polarity head is not used during inference or evaluation.\n",
    "\n",
    "This approach corresponds to task-specific supervised fine-tuning,\n",
    "not prompting and not instruction fine-tuning. Model parameters are\n",
    "updated using labeled examples and cross-entropy loss, and performance\n",
    "is evaluated using macro-averaged F1 on emotion labels.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db02edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "PROJECT_MARKERS = (\"src\", \"data\", \"prompts\", \"results\")\n",
    "\n",
    "def find_project_root(start_path):\n",
    "    current = os.path.abspath(start_path)\n",
    "\n",
    "    while True:\n",
    "        if all(os.path.isdir(os.path.join(current, m)) for m in PROJECT_MARKERS):\n",
    "            return current\n",
    "\n",
    "        parent = os.path.dirname(current)\n",
    "        if parent == current:\n",
    "            raise RuntimeError(\"Project root not found\")\n",
    "\n",
    "        current = parent\n",
    "\n",
    "\n",
    "# ---- execution directory (cwd) ----\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# ---- safe starting point ----\n",
    "try:\n",
    "    start_path = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    start_path = cwd\n",
    "\n",
    "\n",
    "# ---- resolve canonical paths ----\n",
    "project_root = find_project_root(start_path)\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "src_root     = os.path.join(project_root, \"src\", \"daniel\", \"gemini\")\n",
    "data_root    = os.path.join(project_root, \"data\", \"MAMS-ACSA\", \"raw\", \"data_jsonl\")\n",
    "schemas_root = os.path.join(project_root, \"data\", \"MAMS-ACSA\", \"raw\", \"data_jsonl\", \"schema\")\n",
    "prompts_root = os.path.join(project_root, \"prompts\", \"daniel\", \"llama\")\n",
    "utils_root   = os.path.join(project_root, \"utils\")\n",
    "results_root = os.path.join(project_root, \"results\", \"daniel\",\"training_results\")\n",
    "\n",
    "print(\n",
    "    f\"cwd          : {cwd}\\n\"\n",
    "    f\"Project root : {project_root}\\n\"\n",
    "    f\"Source root  : {src_root}\\n\"\n",
    "    f\"Data root    : {data_root}\\n\"\n",
    "    f\"Prompts root : {prompts_root}\\n\"\n",
    "    f\"Utils root   : {utils_root}\\n\"\n",
    "    f\"Results root : {results_root}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3d2e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d90730",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = Path(data_root) / \"train.jsonl\"   # adjust filename \n",
    "\n",
    "rows = []\n",
    "with open(data_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        rows.append(json.loads(line))\n",
    "\n",
    "len(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1720e806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def light_check(rows, name):\n",
    "    for i, r in enumerate(rows):\n",
    "        assert isinstance(r, dict), f\"{name}[{i}] is not a dict\"\n",
    "\n",
    "        # input\n",
    "        assert \"input\" in r, f\"{name}[{i}] missing 'input'\"\n",
    "        assert isinstance(r[\"input\"], str), f\"{name}[{i}]['input'] not a string\"\n",
    "        assert r[\"input\"].strip(), f\"{name}[{i}] empty 'input'\"\n",
    "\n",
    "        # output\n",
    "        assert \"output\" in r, f\"{name}[{i}] missing 'output'\"\n",
    "        assert isinstance(r[\"output\"], list), f\"{name}[{i}]['output'] not a list\"\n",
    "        assert len(r[\"output\"]) > 0, f\"{name}[{i}] empty 'output' list\"\n",
    "\n",
    "        # each label item\n",
    "        for j, o in enumerate(r[\"output\"]):\n",
    "            assert isinstance(o, dict), f\"{name}[{i}]['output'][{j}] not a dict\"\n",
    "            for k in (\"aspect\", \"polarity\", \"emotion\"):\n",
    "                assert k in o, f\"{name}[{i}]['output'][{j}] missing '{k}'\"\n",
    "                assert isinstance(o[k], str), f\"{name}[{i}]['output'][{j}]['{k}'] not a string\"\n",
    "                assert o[k].strip(), f\"{name}[{i}]['output'][{j}] empty '{k}'\"\n",
    "\n",
    "    print(f\"{name}: {len(rows)} rows passed\")\n",
    "\n",
    "\n",
    "light_check(rows=rows, name=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400a8efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand rows because each input can have multiple aspect-emotion-polarity labels\n",
    "records = []\n",
    "\n",
    "for r in rows:\n",
    "    text = r[\"input\"]\n",
    "    for o in r[\"output\"]:\n",
    "        records.append({\n",
    "            \"text\": text,\n",
    "            \"aspect\": o[\"aspect\"],\n",
    "            \"polarity\": o[\"polarity\"],\n",
    "            #\"emotion\": o[\"emotion\"],\n",
    "            \n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b0e3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"emotion\"] = df[\"emotion\"].replace({\"mentioned_only\": \"neutral\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5db2b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"emotion\"].value_counts() # This shows the distribution of emotion labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea949a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_encoder = LabelEncoder()\n",
    "polarity_encoder = LabelEncoder()\n",
    "\n",
    "df[\"emotion_id\"]  = emotion_encoder.fit_transform(df[\"emotion\"])\n",
    "df[\"polarity_id\"] = polarity_encoder.fit_transform(df[\"polarity\"])\n",
    "\n",
    "num_emotions  = len(emotion_encoder.classes_)\n",
    "num_polarity  = len(polarity_encoder.classes_)\n",
    "\n",
    "emotion_encoder.classes_, polarity_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79b722d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_texts = df[\"text\"].unique()\n",
    "\n",
    "train_texts, val_texts = train_test_split(\n",
    "    unique_texts,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_df = df[df[\"text\"].isin(train_texts)]\n",
    "val_df   = df[df[\"text\"].isin(val_texts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca152a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"distilroberta-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f1ca7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for aspect-conditioned emotion and polarity classification.\n",
    "\n",
    "    This dataset converts a tabular representation of reviews into model-ready\n",
    "    tensors. Each item corresponds to a single (text, aspect) pair and produces:\n",
    "\n",
    "    - tokenized input text, where the aspect is explicitly prepended to guide\n",
    "      the model's attention,\n",
    "    - attention masks for the transformer encoder,\n",
    "    - numeric labels for emotion (primary task),\n",
    "    - numeric labels for polarity (auxiliary task).\n",
    "\n",
    "    The dataset is used by the HuggingFace Trainer to dynamically construct\n",
    "    training and evaluation batches during fine-tuning.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, tokenizer, max_len=256):\n",
    "        \n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.loc[idx]\n",
    "\n",
    "        # We explicitly prepend the aspect to the input so the model knows\n",
    "        # *which part* of the sentence it should evaluate. This allows the\n",
    "        # same sentence to yield different emotions depending on the aspect\n",
    "        # (e.g., \"food\" vs. \"service\"), which is essential for aspect-based\n",
    "        # emotion classification.\n",
    "        text = f\"ASPECT: {row['aspect']} | TEXT: {row['text']}\"\n",
    "\n",
    "        enc = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
    "            \"emotion_labels\": torch.tensor(row[\"emotion_id\"]),\n",
    "            \"polarity_labels\": torch.tensor(row[\"polarity_id\"])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00506da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionPolarityModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-task transformer model for aspect-conditioned emotion classification.\n",
    "\n",
    "    The model uses a shared pretrained language encoder to read the input text\n",
    "    and learns two task-specific heads:\n",
    "    - Emotion classification (primary task)\n",
    "    - Polarity classification (auxiliary task)\n",
    "\n",
    "    Polarity supervision is used only to regularize training. During evaluation,\n",
    "    only emotion predictions are considered.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name, num_emotions, num_polarity):\n",
    "        super().__init__()\n",
    "\n",
    "        # Load a pretrained transformer encoder (e.g., DistilRoBERTa).\n",
    "        # This encoder already understands language structure and semantics.\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        # Dimensionality of the encoder's hidden representations\n",
    "        hidden = self.encoder.config.hidden_size\n",
    "\n",
    "        # Linear classification head for emotion prediction (main objective)\n",
    "        self.emotion_head = torch.nn.Linear(hidden, num_emotions)\n",
    "\n",
    "        # Linear classification head for polarity prediction (auxiliary objective)\n",
    "        self.polarity_head = torch.nn.Linear(hidden, num_polarity)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        attention_mask,\n",
    "        emotion_labels=None,\n",
    "        polarity_labels=None\n",
    "    ):\n",
    "        # Pass tokenized input through the transformer encoder\n",
    "        # Output contains contextual representations for each token\n",
    "        out = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        # Use the representation of the first token as a summary of the sequence.\n",
    "        # This vector encodes the overall meaning of the (aspect-conditioned) input.\n",
    "        cls = out.last_hidden_state[:, 0]\n",
    "\n",
    "        # Compute raw (unnormalized) prediction scores for each task\n",
    "        emotion_logits = self.emotion_head(cls)\n",
    "        polarity_logits = self.polarity_head(cls)\n",
    "\n",
    "        loss = None\n",
    "        if emotion_labels is not None:\n",
    "            # Cross-entropy loss for emotion classification (primary task)\n",
    "            loss_e = torch.nn.functional.cross_entropy(\n",
    "                emotion_logits, emotion_labels\n",
    "            )\n",
    "\n",
    "            # Cross-entropy loss for polarity classification (auxiliary task)\n",
    "            loss_p = torch.nn.functional.cross_entropy(\n",
    "                polarity_logits, polarity_labels\n",
    "            )\n",
    "\n",
    "            # Joint loss: emotion dominates, polarity acts as a regularizer\n",
    "            loss = loss_e + 0.3 * loss_p\n",
    "\n",
    "        # Return a dictionary so HuggingFace Trainer can:\n",
    "        # - use 'loss' for backpropagation\n",
    "        # - access logits for metric computation\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"emotion_logits\": emotion_logits,\n",
    "            \"polarity_logits\": polarity_logits\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4840438d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch Dataset objects from the processed dataframes.\n",
    "# These datasets handle aspect-conditioning, tokenization, and\n",
    "# conversion of text and labels into tensors that the Trainer\n",
    "# can batch and feed into the model during training and validation.\n",
    "train_ds = EmotionDataset(train_df, tokenizer)\n",
    "val_ds   = EmotionDataset(val_df, tokenizer)\n",
    "\n",
    "# Initialize the multi-task transformer model.\n",
    "# The pretrained encoder is loaded using MODEL_NAME, and the sizes\n",
    "# of the emotion and polarity classification heads are set according\n",
    "# to the number of unique labels observed in the training data.\n",
    "model = EmotionPolarityModel(\n",
    "    MODEL_NAME,\n",
    "    num_emotions=num_emotions,\n",
    "    num_polarity=num_polarity\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd281c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute evaluation metrics during validation and testing.\n",
    "# This function converts model output logits into discrete class\n",
    "# predictions and computes the macro-averaged F1 score, which\n",
    "# treats all emotion classes equally and is appropriate for\n",
    "# imbalanced emotion distributions.\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "\n",
    "    # Select the class with the highest predicted score for each example\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "\n",
    "    # Return macro-F1 as the primary evaluation metric\n",
    "    return {\n",
    "        \"macro_f1\": f1_score(labels, preds, average=\"macro\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1205e16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Custom Trainer that supports joint emotion–polarity training\n",
    "    while evaluating only emotion predictions.\n",
    "\n",
    "    This class overrides the default HuggingFace Trainer behavior to:\n",
    "    - pass both emotion and polarity labels during training,\n",
    "    - use a custom joint loss defined inside the model,\n",
    "    - restrict evaluation outputs to emotion logits only.\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # Forward pass through the model with both emotion and polarity labels.\n",
    "        # The model internally computes a joint loss where emotion is the\n",
    "        # primary task and polarity acts as an auxiliary regularizer.\n",
    "        outputs = model(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            emotion_labels=inputs[\"emotion_labels\"],\n",
    "            polarity_labels=inputs[\"polarity_labels\"]\n",
    "        )\n",
    "\n",
    "        # Return only the loss (or loss + outputs if required by Trainer)\n",
    "        return (outputs[\"loss\"], outputs) if return_outputs else outputs[\"loss\"]\n",
    "\n",
    "    def prediction_step(\n",
    "        self,\n",
    "        model,\n",
    "        inputs,\n",
    "        prediction_loss_only=False,\n",
    "        ignore_keys=None\n",
    "    ):\n",
    "        # Disable gradient computation during evaluation for efficiency\n",
    "        with torch.no_grad():\n",
    "            # Forward pass WITHOUT labels.\n",
    "            # This ensures predictions are not influenced by supervision.\n",
    "            outputs = model(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"]\n",
    "            )\n",
    "\n",
    "        # Extract emotion logits only.\n",
    "        # Polarity predictions are intentionally ignored during evaluation.\n",
    "        logits = outputs[\"emotion_logits\"]\n",
    "\n",
    "        # Ground-truth emotion labels for metric computation\n",
    "        labels = inputs[\"emotion_labels\"]\n",
    "\n",
    "        # Return (loss, logits, labels) in the format expected by Trainer.\n",
    "        # Loss is None because evaluation loss is not required here.\n",
    "        return None, logits.cpu().numpy(), labels.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47000df",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=results_root,\n",
    "    # Where checkpoints and logs would be written (even if saving is disabled)\n",
    "\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    # Run evaluation on the validation set at the end of every epoch.\n",
    "    # This gives a stable signal of learning progress without over-evaluating.\n",
    "\n",
    "    save_strategy=\"no\",\n",
    "    # Disable checkpoint saving.\n",
    "    # This is intentional for experimentation and avoids disk clutter.\n",
    "\n",
    "    learning_rate=2e-5,\n",
    "    # Standard fine-tuning learning rate for transformer models.\n",
    "    # Low enough to preserve pretrained knowledge, high enough to adapt.\n",
    "\n",
    "    per_device_train_batch_size=16,\n",
    "    # Number of training samples per GPU/CPU step.\n",
    "    # 16 is a safe default for DistilRoBERTa on most hardware.\n",
    "\n",
    "    per_device_eval_batch_size=16,\n",
    "    # Same batch size for evaluation to keep memory usage predictable.\n",
    "\n",
    "    num_train_epochs=5,\n",
    "    # Small number of epochs is sufficient when fine-tuning pretrained models.\n",
    "    # Prevents overfitting on a relatively small dataset (~3k examples).\n",
    "\n",
    "    logging_steps=50,\n",
    "    # Log training metrics every 50 steps for monitoring convergence.\n",
    "\n",
    "    report_to=\"none\"\n",
    "    # Disable external logging frameworks (e.g., WandB, TensorBoard).\n",
    ")\n",
    "\n",
    "trainer = EmotionTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    # Dataset providing (text + aspect) → emotion + polarity supervision\n",
    "\n",
    "    eval_dataset=val_ds,\n",
    "    # Validation set used only for emotion evaluation metrics\n",
    "\n",
    "    compute_metrics=compute_metrics\n",
    "    # Computes macro-F1 on emotion labels after each evaluation\n",
    ")\n",
    "\n",
    "# Start fine-tuning the pretrained transformer model\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
