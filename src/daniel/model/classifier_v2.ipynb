{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a4cc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We fine-tune a pretrained transformer encoder to perform aspect-based emotion classification.”\n",
    "\"\"\"\n",
    "Modeling Rationale\n",
    "\n",
    "Although classical machine learning models such as Logistic Regression or\n",
    "Support Vector Machines can be trained from scratch for text classification,\n",
    "they rely on shallow lexical representations (e.g., bag-of-words or TF–IDF)\n",
    "and therefore lack an explicit notion of context, word order, and semantic\n",
    "composition.\n",
    "\n",
    "Emotion classification, and especially aspect-based emotion classification,\n",
    "is inherently context-dependent. The same sentence may express different\n",
    "emotions depending on the referenced aspect, and emotional meaning is often\n",
    "conveyed implicitly through contrast, expectation, or discourse structure\n",
    "rather than through isolated keywords.\n",
    "\n",
    "Training a model from scratch under these conditions would require learning\n",
    "both linguistic structure and emotional semantics simultaneously, which is\n",
    "impractical with limited labeled data. Instead, we fine-tune a pretrained\n",
    "transformer encoder that already captures rich syntactic and semantic\n",
    "regularities of natural language. Fine-tuning allows the model to adapt this\n",
    "general linguistic knowledge to the specific task of emotion classification\n",
    "with substantially fewer examples and improved generalization.\n",
    "\n",
    "For these reasons, a fine-tuned transformer-based classifier is more suitable\n",
    "than classical models trained from scratch for the task considered here.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc66fb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model Choice Justification\n",
    "\n",
    "We use DistilRoBERTa as the underlying language encoder for emotion classification.\n",
    "DistilRoBERTa is a distilled version of RoBERTa that retains most of its\n",
    "representational power while being significantly smaller and faster. This makes\n",
    "it well-suited for experimental settings with limited computational resources\n",
    "and moderate-sized datasets, while still providing strong contextual language\n",
    "understanding.\n",
    "\n",
    "RoBERTa-based models are particularly effective for sentiment and emotion-related\n",
    "tasks because they are pretrained on large amounts of diverse text using a\n",
    "masked language modeling objective that captures nuanced lexical, syntactic,\n",
    "and semantic relationships. These properties are critical for emotion detection,\n",
    "where affect is often expressed implicitly rather than through explicit emotion\n",
    "words.\n",
    "\n",
    "DistilRoBERTa offers a practical trade-off between performance and efficiency:\n",
    "it enables stable fine-tuning, faster iteration, and reduced overfitting risk\n",
    "compared to larger models, without sacrificing the core benefits of contextual\n",
    "representations.\n",
    "\n",
    "Alternative pretrained models could also be used within the same framework.\n",
    "For example:\n",
    "- BERT-base can serve as a widely adopted baseline, though it is generally\n",
    "  weaker than RoBERTa-style models on sentiment-oriented tasks.\n",
    "- RoBERTa-base may yield slightly higher performance at the cost of increased\n",
    "  memory usage and slower training.\n",
    "- ALBERT provides parameter sharing for efficiency but can be less stable during\n",
    "  fine-tuning.\n",
    "- Domain-specific models (e.g., sentiment-adapted or review-trained transformers)\n",
    "  may further improve performance if available.\n",
    "\n",
    "The choice of DistilRoBERTa therefore reflects a deliberate balance between model\n",
    "capacity, training stability, computational efficiency, and suitability for\n",
    "aspect-based emotion classification.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9042da7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "PROJECT_MARKERS = (\"src\", \"data\", \"prompts\", \"results\")\n",
    "\n",
    "def find_project_root(start_path):\n",
    "    current = os.path.abspath(start_path)\n",
    "\n",
    "    while True:\n",
    "        if all(os.path.isdir(os.path.join(current, m)) for m in PROJECT_MARKERS):\n",
    "            return current\n",
    "\n",
    "        parent = os.path.dirname(current)\n",
    "        if parent == current:\n",
    "            raise RuntimeError(\"Project root not found\")\n",
    "\n",
    "        current = parent\n",
    "\n",
    "\n",
    "# ---- execution directory (cwd) ----\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# ---- safe starting point ----\n",
    "try:\n",
    "    start_path = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    start_path = cwd\n",
    "\n",
    "\n",
    "# ---- resolve canonical paths ----\n",
    "project_root = find_project_root(start_path)\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "src_root     = os.path.join(project_root, \"src\", \"daniel\", \"gemini\")\n",
    "data_root    = os.path.join(project_root, \"data\", \"MAMS-ACSA\", \"raw\", \"data_jsonl\")\n",
    "schemas_root = os.path.join(project_root, \"data\", \"MAMS-ACSA\", \"raw\", \"data_jsonl\", \"schema\")\n",
    "prompts_root = os.path.join(project_root, \"prompts\", \"daniel\", \"llama\")\n",
    "utils_root   = os.path.join(project_root, \"utils\")\n",
    "results_root = os.path.join(project_root, \"results\", \"daniel\",\"training_results\")\n",
    "\n",
    "print(\n",
    "    f\"cwd          : {cwd}\\n\"\n",
    "    f\"Project root : {project_root}\\n\"\n",
    "    f\"Source root  : {src_root}\\n\"\n",
    "    f\"Data root    : {data_root}\\n\"\n",
    "    f\"Prompts root : {prompts_root}\\n\"\n",
    "    f\"Utils root   : {utils_root}\\n\"\n",
    "    f\"Results root : {results_root}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490127d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb20b5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(path):\n",
    "    rows = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "train_rows = load_jsonl(Path(data_root) / \"train.jsonl\")\n",
    "val_rows   = load_jsonl(Path(data_root) / \"val.jsonl\")\n",
    "test_rows  = load_jsonl(Path(data_root) / \"test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5840c3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 3149 rows passed\n",
      "val: 400 rows passed\n",
      "test: 400 rows passed\n"
     ]
    }
   ],
   "source": [
    "def light_check(rows, name):\n",
    "    for i, r in enumerate(rows):\n",
    "        assert isinstance(r, dict), f\"{name}[{i}] is not a dict\"\n",
    "\n",
    "        # input\n",
    "        assert \"input\" in r, f\"{name}[{i}] missing 'input'\"\n",
    "        assert isinstance(r[\"input\"], str), f\"{name}[{i}]['input'] not a string\"\n",
    "        assert r[\"input\"].strip(), f\"{name}[{i}] empty 'input'\"\n",
    "\n",
    "        # output\n",
    "        assert \"output\" in r, f\"{name}[{i}] missing 'output'\"\n",
    "        assert isinstance(r[\"output\"], list), f\"{name}[{i}]['output'] not a list\"\n",
    "        assert len(r[\"output\"]) > 0, f\"{name}[{i}] empty 'output' list\"\n",
    "\n",
    "        # # each label item\n",
    "        # for j, o in enumerate(r[\"output\"]):\n",
    "        #     assert isinstance(o, dict), f\"{name}[{i}]['output'][{j}] not a dict\"\n",
    "        #     for k in (\"aspect\", \"polarity\", \"emotion\"):\n",
    "        #         assert k in o, f\"{name}[{i}]['output'][{j}] missing '{k}'\"\n",
    "        #         assert isinstance(o[k], str), f\"{name}[{i}]['output'][{j}]['{k}'] not a string\"\n",
    "        #         assert o[k].strip(), f\"{name}[{i}]['output'][{j}] empty '{k}'\"\n",
    "\n",
    "    print(f\"{name}: {len(rows)} rows passed\")\n",
    "\n",
    "\n",
    "light_check(train_rows, \"train\")\n",
    "light_check(val_rows, \"val\")\n",
    "light_check(test_rows, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491b72c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explode(rows):\n",
    "    records = []\n",
    "    for r in rows:\n",
    "        text = r[\"input\"]\n",
    "        for o in r[\"output\"]:\n",
    "            records.append({\n",
    "                \"text\": text,\n",
    "                \"aspect\": o[\"aspect\"],\n",
    "                \"polarity\": o[\"polarity\"],\n",
    "                \"emotion\": o[\"emotion\"]\n",
    "                \n",
    "            })\n",
    "    return pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47396c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = explode(train_rows)\n",
    "val_df   = explode(val_rows)\n",
    "test_df  = explode(test_rows)\n",
    "\n",
    "len(train_df), len(val_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab821c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in (train_df, val_df, test_df):\n",
    "    df[\"emotion\"] = df[\"emotion\"].replace({\"mentioned_only\": \"neutral\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a179ac98",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"emotion\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae4bbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_encoder = LabelEncoder()\n",
    "polarity_encoder = LabelEncoder()\n",
    "\n",
    "# fit on TRAIN only\n",
    "train_df[\"emotion_id\"]  = emotion_encoder.fit_transform(train_df[\"emotion\"])\n",
    "train_df[\"polarity_id\"] = polarity_encoder.fit_transform(train_df[\"polarity\"])\n",
    "\n",
    "# apply to VAL / TEST\n",
    "val_df[\"emotion_id\"]  = emotion_encoder.transform(val_df[\"emotion\"])\n",
    "val_df[\"polarity_id\"] = polarity_encoder.transform(val_df[\"polarity\"])\n",
    "\n",
    "test_df[\"emotion_id\"]  = emotion_encoder.transform(test_df[\"emotion\"])\n",
    "test_df[\"polarity_id\"] = polarity_encoder.transform(test_df[\"polarity\"])\n",
    "\n",
    "emotion_encoder.classes_, polarity_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812fa117",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=256):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.loc[idx]\n",
    "\n",
    "        text = f\"ASPECT: {row['aspect']} | TEXT: {row['text']}\"\n",
    "\n",
    "        enc = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
    "            \"emotion_labels\": torch.tensor(row[\"emotion_id\"]),\n",
    "            \"polarity_labels\": torch.tensor(row[\"polarity_id\"]),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79a4c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"distilroberta-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "train_ds = EmotionDataset(train_df, tokenizer)\n",
    "val_ds   = EmotionDataset(val_df, tokenizer)\n",
    "test_ds  = EmotionDataset(test_df, tokenizer)\n",
    "\n",
    "len(train_ds), len(val_ds), len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f7de21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionPolarityModel(torch.nn.Module):\n",
    "    def __init__(self, model_name, num_emotions, num_polarity):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        hidden_size = self.encoder.config.hidden_size\n",
    "\n",
    "        self.emotion_head  = torch.nn.Linear(hidden_size, num_emotions)\n",
    "        self.polarity_head = torch.nn.Linear(hidden_size, num_polarity)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        attention_mask,\n",
    "        emotion_labels=None,\n",
    "        polarity_labels=None\n",
    "    ):\n",
    "        outputs = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        cls_repr = outputs.last_hidden_state[:, 0]\n",
    "\n",
    "        emotion_logits  = self.emotion_head(cls_repr)\n",
    "        polarity_logits = self.polarity_head(cls_repr)\n",
    "\n",
    "        loss = None\n",
    "        if emotion_labels is not None:\n",
    "            loss_emotion = torch.nn.functional.cross_entropy(\n",
    "                emotion_logits, emotion_labels\n",
    "            )\n",
    "            loss_polarity = torch.nn.functional.cross_entropy(\n",
    "                polarity_logits, polarity_labels\n",
    "            )\n",
    "            loss = loss_emotion + 0.3 * loss_polarity\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"emotion_logits\": emotion_logits,\n",
    "            \"polarity_logits\": polarity_logits,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b906d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "\n",
    "    emotion_logits = logits[\"emotion_logits\"]\n",
    "    emotion_labels = labels[\"emotion_labels\"]\n",
    "\n",
    "    preds = np.argmax(emotion_logits, axis=1)\n",
    "\n",
    "    return {\n",
    "        \"emotion_f1_macro\": f1_score(\n",
    "            emotion_labels,\n",
    "            preds,\n",
    "            average=\"macro\"\n",
    "        )\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2db23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EmotionPolarityModel(\n",
    "    model_name=MODEL_NAME,\n",
    "    num_emotions=len(emotion_encoder.classes_),\n",
    "    num_polarity=len(polarity_encoder.classes_)\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=results_root,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab627cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
